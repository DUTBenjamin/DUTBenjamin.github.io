<!DOCTYPE html>
<html data-color-mode="light" data-dark-theme="dark" data-light-theme="light" lang="zh-CN">
<head>
    <meta content="text/html; charset=utf-8" http-equiv="content-type" />
    <meta name="viewport" content="width=device-width,initial-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <link href='https://mirrors.sustech.edu.cn/cdnjs/ajax/libs/Primer/21.0.7/primer.css' rel='stylesheet' />
    
    <link rel="icon" href="https://avatars.githubusercontent.com/u/165814040?v=4"><script>
        let theme = localStorage.getItem("meek_theme") || "light";
        document.documentElement.setAttribute("data-color-mode", theme);
    </script>
<meta name="description" content="# 神经网络基础笔记
## 神经网络发展史
### 感知器时代（1950s - 1970s）
- 1958年，感知机（Perceptron）被提出，是线性二分类器，有两层神经元，是首个能学习权重并进行简单分类的人工神经网络，具备现今神经网络主要构件与思想，但无法解决异或问题（数据线性不可分问题），导致神经网络进入第一次寒冬。">
<meta property="og:title" content="机器学习_12_CNN">
<meta property="og:description" content="# 神经网络基础笔记
## 神经网络发展史
### 感知器时代（1950s - 1970s）
- 1958年，感知机（Perceptron）被提出，是线性二分类器，有两层神经元，是首个能学习权重并进行简单分类的人工神经网络，具备现今神经网络主要构件与思想，但无法解决异或问题（数据线性不可分问题），导致神经网络进入第一次寒冬。">
<meta property="og:type" content="article">
<meta property="og:url" content="https://DUTBenjamin.github.io/post/ji-qi-xue-xi-_12_CNN.html">
<meta property="og:image" content="https://avatars.githubusercontent.com/u/165814040?v=4">
<title>机器学习_12_CNN</title>



</head>
<style>
body{box-sizing: border-box;min-width: 200px;max-width: 900px;margin: 20px auto;padding: 45px;font-size: 16px;font-family: sans-serif;line-height: 1.25;}
#header{display:flex;padding-bottom:8px;border-bottom: 1px solid var(--borderColor-muted, var(--color-border-muted));margin-bottom: 16px;}
#footer {margin-top:64px; text-align: center;font-size: small;}

</style>

<style>
.postTitle{margin: auto 0;font-size:40px;font-weight:bold;}
.title-right{display:flex;margin:auto 0 0 auto;}
.title-right .circle{padding: 14px 16px;margin-right:8px;}
#postBody{border-bottom: 1px solid var(--color-border-default);padding-bottom:36px;}
#postBody hr{height:2px;}
#cmButton{height:48px;margin-top:48px;}
#comments{margin-top:64px;}
.g-emoji{font-size:24px;}
@media (max-width: 600px) {
    body {padding: 8px;}
    .postTitle{font-size:24px;}
}

</style>




<body>
    <div id="header">
<h1 class="postTitle">机器学习_12_CNN</h1>
<div class="title-right">
    <a href="https://DUTBenjamin.github.io" id="buttonHome" class="btn btn-invisible circle" title="首页">
        <svg class="octicon" width="16" height="16">
            <path id="pathHome" fill-rule="evenodd"></path>
        </svg>
    </a>
    
    <a href="https://github.com/DUTBenjamin/DUTBenjamin.github.io/issues/1" target="_blank" class="btn btn-invisible circle" title="Issue">
        <svg class="octicon" width="16" height="16">
            <path id="pathIssue" fill-rule="evenodd"></path>
        </svg>
    </a>
    

    <a class="btn btn-invisible circle" onclick="modeSwitch();" title="切换主题">
        <svg class="octicon" width="16" height="16" >
            <path id="themeSwitch" fill-rule="evenodd"></path>
        </svg>
    </a>

</div>
</div>
    <div id="content">
<div class="markdown-body" id="postBody"><h1>神经网络基础笔记</h1>
<h2>神经网络发展史</h2>
<h3>感知器时代（1950s - 1970s）</h3>
<ul>
<li>1958年，感知机（Perceptron）被提出，是线性二分类器，有两层神经元，是首个能学习权重并进行简单分类的人工神经网络，具备现今神经网络主要构件与思想，但无法解决异或问题（数据线性不可分问题），导致神经网络进入第一次寒冬。</li>
<li>1980s，多层感知机出现，与现在的深度神经网络（DNN）无显著区别。</li>
</ul>
<h3>BP算法时代（1986 - 1998）</h3>
<ul>
<li>1986年，反向传播（BP）算法提出，引入激活函数sigmoid，解决感知机遗留问题，实现多层网络BP算法，是应用最广泛的训练更新算法，但一般大于3个隐藏层作用不大。</li>
<li>1989年，提出1个隐藏层“足够好”，引发对深度必要性的思考。LeNet被提出，为后续深度学习研究奠基，促进卷积神经网络发展。</li>
<li>1991年，BP算法被指存在梯度消失和爆炸问题，神经网络发展受冷遇。</li>
<li>1990s，支持向量机（SVM）出现，理论完备、可解释性强，解决神经网络部分遗留问题，使其再次冷寂。1997年，长短期记忆网络（LSTM）被提出。1998年，基于LeNet的LeNet - 5提出，实现手写数字识别，在MNIST数据集上表现受关注。</li>
</ul>
<h3>深度学习时代（2006 - 至今）</h3>
<ul>
<li>2000s，GPU和分布式计算助力计算机算力提升，为深度学习奠定基础。</li>
<li>2006年，深度信念网络（DBN）取得突破，利用无监督预训练解决深层神经网络梯度消失问题，深度学习时代来临。</li>
<li>2009年，受限玻尔兹曼机（RBM）提出，可学习数据集概率分布。2011年，开始在语音识别领域流行，同年ReLu激活函数被提出，解决Sigmoid函数梯度消失问题，至今广泛应用。</li>
<li>2012年，AlexNet在ImageNet图像分类大赛中获胜，推动深度学习快速发展。</li>
<li>2014年，VGGNet、GoogLeNet被提出，GoogLeNet提出inception模块学习不同尺度特征；同年生成对抗网络（GAN）被提出。</li>
<li>2015年，残差网络（ResNet）提出，残差思想突破，在ImageNet比赛中夺冠，图像识别超越人类水平。</li>
<li>2016年，轻量级卷积神经网络SqueezeNet、Alpha GO击败围棋世界冠军、在语音识别取得进展。</li>
<li>2017年，ResNet升级版DenseNet、Google提出Transformer模型（基于自注意力机制，为后续BERT、GPT等模型奠基）被提出。</li>
<li>2018年，Google提出BERT模型，显著提升多项自然语言处理（NLP）任务性能。</li>
<li>2019年，OpenAI提出GPT - 2模型引发关注，DeepMind的AlphaStar在星际争霸II中战胜人类职业选手，AlphaFold解决蛋白质折叠问题。</li>
<li>2020年，DeepMind的AlphaFold解决蛋白质折叠问题，获生物学领域突破。</li>
<li>2021年，OpenAI发布图像生成模型DALL·E。</li>
<li>2022年，OpenAI发布GPT - 3.5（ChatGPT），引发大模型浪潮。</li>
<li>2023年，OpenAI发布ChatGLM，具备对话能力的聊天机器人。</li>
<li>2024年，OpenAI发布Sora，诺贝尔物理学奖、化学奖颁发给神经网络领域相关科学家。</li>
</ul>
<h2>神经元和神经网络</h2>
<h3>深度学习步骤</h3>
<ol>
<li><strong>神经网络（Neural Network）</strong>：由输入层、隐藏层（可多层）和输出层构成，不同连接方式形成不同网络结构，网络参数包括所有权重和偏置。从线性回归到逻辑回归，逻辑回归输出在0 - 1之间，通过激活函数（如Sigmoid函数）实现，常用全连接前馈网络，可使用GPU加速计算。</li>
<li><strong>模型评估（Goodness of Function）</strong>：通过损失函数（如hinge loss、cross entropy loss等）评估模型好坏。</li>
<li><strong>选择最优函数（Pick the Best Function）</strong>：使用梯度下降法（包括随机梯度下降）等优化算法，通过计算损失函数对参数的梯度，更新参数找到最优函数。</li>
</ol>
<h3>神经网络结构与计算</h3>
<ul>
<li>神经网络通过神经元连接，每个神经元接收输入，乘以权重并加上偏置后，经过激活函数处理得到输出，多层神经元连接形成网络结构。</li>
<li>全连接前馈网络中，输入数据依次经过各隐藏层处理后到达输出层，输入层神经元数量取决于输入数据维度，输出层神经元数量取决于任务类别数，隐藏层数量和每层神经元数量需通过试验、直觉或特定算法确定（如卷积神经网络CNN可自动确定部分结构），也可使用神经架构搜索（NAS）自动寻找最优结构，NAS包括搜索空间、搜索策略和性能评估策略三部分。</li>
</ul>
<h3>示例应用（以手写数字识别为例）</h3>
<ul>
<li>输入为256维向量（16x16像素图像，有墨为1，无墨为0），输出为10维向量（代表0 - 9十个数字的概率），需确定网络结构，使函数集中包含能准确分类的函数。</li>
</ul>
<h2>反向传播算法</h2>
<h3>梯度下降法</h3>
<ol>
<li>梯度下降法是深度学习中优化神经网络参数的常用方法，通过计算损失函数对参数的梯度，沿梯度反方向更新参数以最小化损失函数。</li>
<li>计算梯度时，需计算损失函数对每个权重（(w)）和偏置（(b)）的偏导数（(\frac{\partial L}{\partial w})、(\frac{\partial L}{\partial b})），形成梯度向量（(\nabla L)），然后根据学习率（(\eta)）更新参数（(\theta^{new}=\theta^{old}-\eta\nabla L)），其中(\theta)表示网络参数（(w)和(b)）。</li>
</ol>
<h3>反向传播（Backpropagation）</h3>
<ol>
<li>反向传播是计算神经网络中损失函数对权重和偏置梯度的有效方法，许多深度学习框架（如TensorFlow、PyTorch等）都基于此实现自动求导。</li>
<li>其原理基于链式法则，对于多层神经网络，从输出层开始，反向计算每层的梯度。计算过程包括前向传播（Forward pass）计算各层输出和激活函数输入（(z)），以及反向传播（Backward pass）计算损失函数对(z)的梯度（(\frac{\partial C}{\partial z})），进而计算对权重和偏置的梯度（(\frac{\partial C}{\partial w})、(\frac{\partial C}{\partial b})）。</li>
<li>前向传播计算参数(z)（如(z = x_1w_1 + x_2w_2 + b)）对权重(w)的偏导数（(\frac{\partial z}{\partial w})），其值等于与该权重相连输入的数值（如(\frac{\partial z}{\partial w_1}=x_1)）。</li>
<li>反向传播计算损失函数对(z)的梯度（(\frac{\partial C}{\partial z})），根据链式法则，对于输出层，(\frac{\partial C}{\partial z}=\frac{\partial y}{\partial z}\frac{\partial C}{\partial y})（如使用softmax和交叉熵损失函数时）；对于非输出层，通过递归计算（如(\frac{\partial C}{\partial z}=\sigma'(z)[w_3\frac{\partial C}{\partial z'}+w_4\frac{\partial C}{\partial z''}])），其中(\sigma'(z))为激活函数的导数，(z')、(z'')为后续层的输入。</li>
</ol>
<h2>CNN简介</h2>
<h3>基本概念</h3>
<ol>
<li>卷积神经网络（CNN）是含卷积计算的前馈神经网络，用于处理具有网格结构数据（如图像、音频），相比全连接网络，有局部连接、权重共享和池化层三个结构特性。</li>
<li>局部连接（感受野）：神经元仅考虑输入图像的局部区域（感受野），不同感受野可重叠，多个神经元可共用同一感受野，感受野大小影响对原始图像特征的感知范围，常见填充方法有填充均值、0或边界值，步长决定感受野移动步幅。</li>
<li>权重共享：不同感受野的神经元可共享参数（通过卷积核或滤波器实现），同一感受野内神经元一般不共享参数，权重共享减少参数数量，但也限制网络灵活性，不同图片因输入不同，即使共享参数输出也不同。</li>
<li>池化层（Pooling）：如最大池化（Max Pooling），对特征图进行下采样，减少数据量，保留主要特征，不改变图像对象特征，池化操作可缩小特征图尺寸，降低计算量，实际中常与卷积层交替使用，但在处理精细数据（如围棋棋盘）时可能不适用（如AlphaGo未使用池化层）。</li>
</ol>
<h3>工作原理</h3>
<ol>
<li>卷积层：通过卷积核（滤波器）在输入图像上滑动进行卷积操作，每个卷积核检测一种小模式（特征），输出特征图，多个卷积核可提取多种特征，卷积层叠加深可扩大网络感受野，检测更大范围特征。</li>
<li>全连接层：通常在网络末端，将卷积层和池化层提取的特征映射到最终输出（如分类任务的类别概率）。</li>
</ol>
<h3>应用场景</h3>
<ol>
<li>图像分类：如在ImageNet数据集上的应用，通过卷积层提取图像特征，池化层下采样，全连接层分类，使用交叉熵损失函数和softmax函数计算预测概率和损失。</li>
<li>围棋：AlphaGo使用CNN，因围棋存在局部特征和模式重复，适合用CNN处理，输入19x19x48图像（48个特征平面），卷积层提取特征，最后一层卷积结合softmax预测落子位置，虽围棋未用池化层，但CNN仍表现出色。</li>
<li>其他：在语音处理、自然语言处理等领域也有应用，如语音中的卷积层处理音频特征，自然语言处理中通过卷积操作提取文本特征。</li>
</ol></div>
<div style="font-size:small;margin-top:8px;float:right;"></div>

<button class="btn btn-block" type="button" onclick="openComments()" id="cmButton">评论</button>
<div class="comments" id="comments"></div>

</div>
    <div id="footer"><div id="footer1">Copyright © <span id="copyrightYear"></span> <a href="https://DUTBenjamin.github.io">DUTBenjamin的博客</a></div>
<div id="footer2">
    <span id="runday"></span><span>Powered by <a href="https://meekdai.com/Gmeek.html" target="_blank">Gmeek</a></span>
</div>

<script>
var now=new Date();
document.getElementById("copyrightYear").innerHTML=now.getFullYear();

if(""!=""){
    var startSite=new Date("");
    var diff=now.getTime()-startSite.getTime();
    var diffDay=Math.floor(diff/(1000*60*60*24));
    document.getElementById("runday").innerHTML="网站运行"+diffDay+"天"+" • ";
}
</script></div>
</body>
<script>
var IconList={'sun': 'M8 10.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5zM8 12a4 4 0 100-8 4 4 0 000 8zM8 0a.75.75 0 01.75.75v1.5a.75.75 0 01-1.5 0V.75A.75.75 0 018 0zm0 13a.75.75 0 01.75.75v1.5a.75.75 0 01-1.5 0v-1.5A.75.75 0 018 13zM2.343 2.343a.75.75 0 011.061 0l1.06 1.061a.75.75 0 01-1.06 1.06l-1.06-1.06a.75.75 0 010-1.06zm9.193 9.193a.75.75 0 011.06 0l1.061 1.06a.75.75 0 01-1.06 1.061l-1.061-1.06a.75.75 0 010-1.061zM16 8a.75.75 0 01-.75.75h-1.5a.75.75 0 010-1.5h1.5A.75.75 0 0116 8zM3 8a.75.75 0 01-.75.75H.75a.75.75 0 010-1.5h1.5A.75.75 0 013 8zm10.657-5.657a.75.75 0 010 1.061l-1.061 1.06a.75.75 0 11-1.06-1.06l1.06-1.06a.75.75 0 011.06 0zm-9.193 9.193a.75.75 0 010 1.06l-1.06 1.061a.75.75 0 11-1.061-1.06l1.06-1.061a.75.75 0 011.061 0z', 'moon': 'M9.598 1.591a.75.75 0 01.785-.175 7 7 0 11-8.967 8.967.75.75 0 01.961-.96 5.5 5.5 0 007.046-7.046.75.75 0 01.175-.786zm1.616 1.945a7 7 0 01-7.678 7.678 5.5 5.5 0 107.678-7.678z', 'sync': 'M1.705 8.005a.75.75 0 0 1 .834.656 5.5 5.5 0 0 0 9.592 2.97l-1.204-1.204a.25.25 0 0 1 .177-.427h3.646a.25.25 0 0 1 .25.25v3.646a.25.25 0 0 1-.427.177l-1.38-1.38A7.002 7.002 0 0 1 1.05 8.84a.75.75 0 0 1 .656-.834ZM8 2.5a5.487 5.487 0 0 0-4.131 1.869l1.204 1.204A.25.25 0 0 1 4.896 6H1.25A.25.25 0 0 1 1 5.75V2.104a.25.25 0 0 1 .427-.177l1.38 1.38A7.002 7.002 0 0 1 14.95 7.16a.75.75 0 0 1-1.49.178A5.5 5.5 0 0 0 8 2.5Z', 'home': 'M6.906.664a1.749 1.749 0 0 1 2.187 0l5.25 4.2c.415.332.657.835.657 1.367v7.019A1.75 1.75 0 0 1 13.25 15h-3.5a.75.75 0 0 1-.75-.75V9H7v5.25a.75.75 0 0 1-.75.75h-3.5A1.75 1.75 0 0 1 1 13.25V6.23c0-.531.242-1.034.657-1.366l5.25-4.2Zm1.25 1.171a.25.25 0 0 0-.312 0l-5.25 4.2a.25.25 0 0 0-.094.196v7.019c0 .138.112.25.25.25H5.5V8.25a.75.75 0 0 1 .75-.75h3.5a.75.75 0 0 1 .75.75v5.25h2.75a.25.25 0 0 0 .25-.25V6.23a.25.25 0 0 0-.094-.195Z', 'github': 'M8 0c4.42 0 8 3.58 8 8a8.013 8.013 0 0 1-5.45 7.59c-.4.08-.55-.17-.55-.38 0-.27.01-1.13.01-2.2 0-.75-.25-1.23-.54-1.48 1.78-.2 3.65-.88 3.65-3.95 0-.88-.31-1.59-.82-2.15.08-.2.36-1.02-.08-2.12 0 0-.67-.22-2.2.82-.64-.18-1.32-.27-2-.27-.68 0-1.36.09-2 .27-1.53-1.03-2.2-.82-2.2-.82-.44 1.1-.16 1.92-.08 2.12-.51.56-.82 1.28-.82 2.15 0 3.06 1.86 3.75 3.64 3.95-.23.2-.44.55-.51 1.07-.46.21-1.61.55-2.33-.66-.15-.24-.6-.83-1.23-.82-.67.01-.27.38.01.53.34.19.73.9.82 1.13.16.45.68 1.31 2.69.94 0 .67.01 1.3.01 1.49 0 .21-.15.45-.55.38A7.995 7.995 0 0 1 0 8c0-4.42 3.58-8 8-8Z'};
var utterancesLoad=0;

let themeSettings={
    "dark": ["dark","moon","#00f0ff","dark-blue"],
    "light": ["light","sun","#ff5000","github-light"],
    "auto": ["auto","sync","","preferred-color-scheme"]
};
function changeTheme(mode, icon, color, utheme){
    document.documentElement.setAttribute("data-color-mode",mode);
    document.getElementById("themeSwitch").setAttribute("d",value=IconList[icon]);
    document.getElementById("themeSwitch").parentNode.style.color=color;
    if(utterancesLoad==1){utterancesTheme(utheme);}
}
function modeSwitch(){
    let currentMode=document.documentElement.getAttribute('data-color-mode');
    let newMode = currentMode === "light" ? "dark" : currentMode === "dark" ? "auto" : "light";
    localStorage.setItem("meek_theme", newMode);
    if(themeSettings[newMode]){
        changeTheme(...themeSettings[newMode]);
    }
}
function utterancesTheme(theme){
    const message={type:'set-theme',theme: theme};
    const iframe=document.getElementsByClassName('utterances-frame')[0];
    iframe.contentWindow.postMessage(message,'https://utteranc.es');
}
if(themeSettings[theme]){changeTheme(...themeSettings[theme]);}
console.log("\n %c Gmeek last https://github.com/Meekdai/Gmeek \n","padding:5px 0;background:#02d81d;color:#fff");
</script>

<script>
document.getElementById("pathHome").setAttribute("d",IconList["home"]);
document.getElementById("pathIssue").setAttribute("d",IconList["github"]);



function openComments(){
    cm=document.getElementById("comments");
    cmButton=document.getElementById("cmButton");
    cmButton.innerHTML="loading";
    span=document.createElement("span");
    span.setAttribute("class","AnimatedEllipsis");
    cmButton.appendChild(span);

    script=document.createElement("script");
    script.setAttribute("src","https://utteranc.es/client.js");
    script.setAttribute("repo","DUTBenjamin/DUTBenjamin.github.io");
    script.setAttribute("issue-term","title");
    
    if(localStorage.getItem("meek_theme")=="dark"){script.setAttribute("theme","dark-blue");}
    else if(localStorage.getItem("meek_theme")=="light") {script.setAttribute("theme","github-light");}
    else{script.setAttribute("theme","preferred-color-scheme");}
    
    script.setAttribute("crossorigin","anonymous");
    script.setAttribute("async","");
    cm.appendChild(script);

    int=self.setInterval("iFrameLoading()",200);
}

function iFrameLoading(){
    var utterances=document.getElementsByClassName('utterances');
    if(utterances.length==1){
        if(utterances[0].style.height!=""){
            utterancesLoad=1;
            int=window.clearInterval(int);
            document.getElementById("cmButton").style.display="none";
            console.log("utterances Load OK");
        }
    }
}



</script>


</html>
