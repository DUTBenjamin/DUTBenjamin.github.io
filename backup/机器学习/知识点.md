# 模式识别与机器学习课程笔记

## 一、课程信息

### （一）课程基本情况

-   **课程名称**：模式识别与机器学习
-   **课程基础**：线性代数、概率与统计、最优化理论
-   **交叉课程**：图像处理、计算机视觉、数据挖掘、自然语言处理
-   **参考资料**
    -   《机器学习》（周志华，清华大学出版社）
    -   《统计学习方法》（李航等，清华大学出版社）
    -   《深度学习》（Ian Goodfellow等，人民邮电出版社）
    -   《模式分类（第2版）》（[美]迪达等著）
    -   《Pattern Recognition and Machine Learning》（Christopher Bishop，Springer）

### （二）授课内容

-   **机器学习基本概念**
-   **传统机器学习算法**
    -   监督学习（如线性回归、支持向量机、逻辑斯蒂回归、Boosting等）
    -   无监督学习（如K-Means、聚类算法、降维算法等）
    -   半监督学习
-   **其他内容**
    -   稀疏表示与低秩矩阵，神经网络与深度学习，强化学习
    -   机器学习前沿：迁移学习，对抗学习

### （三）教学日历

| 节次 | 教学内容                                             |
|------|------------------------------------------------------|
| 1    | 第一章 绪论 - 机器学习发展沿革和基本术语             |
| 2    | 第一章 绪论 - 模型评估与选择方法                     |
| 3    | 第二章 线性模型 - 基本形式、回归问题                 |
| 4    | 第二章 线性模型 - 判别分析、多分类问题               |
| 5    | 第三章 支持向量机 - 间隔、支持向量、对偶问题、核函数 |
| 6    | 第三章 支持向量机 - 软间隔与正则化、SVM回归          |
| 7    | 第四章 决策树 - 基本流程、划分策略、剪枝             |
| 8    | 第四章 决策树 - 连续值处理、缺失值处理、多变量决策树 |
| 9    | 第五章 集成学习 - 个体与集成，bagging和随机森林      |
| 10   | 第五章 集成学习 - AdaBoost算法、结合策略、多样性     |
| 11   | 第六章 聚类 - 高斯混合聚类                           |
| 12   | 第六章 聚类                                          |
| 13   | 第七章 降维方法 - 基本概念、PCA、度量学习            |
| 14   | 第七章 降维方法 - 非线性降维、流形学习               |
| 15   | 第八章 半监督学习 - 基本概念、半监督SVM              |
| 16   | 第八章 半监督学习 - 基于分歧的方法、半监督聚类       |
| 17   | 第九章 贝叶斯分类器 - 叶斯决策论、朴素贝叶斯分类器   |
| 18   | 第九章 贝叶斯分类器 - 图模型基本概念、贝叶斯网       |
| 19   | 第十章 概率图模型 - HMM                              |
| 20   | 第十一章 CV领域特征描述子 - HOG                      |
| 21   | 第十一章 CV领域特征描述子 - SIFT                     |
| 22   | 第十二章 神经网络基础 - 前馈神经网络与反向传播算法   |
| 23   | 第十二章 神经网络基础 - CNN简介                      |
| 24   | 第十三章 机器学习方法的应用&复习                     |

### （四）考核方式

-   **笔试（40%）**：闭卷，考试时间待定
-   **平时作业（20%）**：课程过程中留4次课后作业，提交时间在金课平台公布，提交方式为电子版/扫描版至金课平台
-   **大作业（40%）**
    -   Poster：具体要求另行通知
    -   Project：编程大作业

## 二、机器学习概述

### （一）定义与发展

-   **定义**：机器学习是人工智能的分支，通过数据学习规律并预测未知数据，涉及多学科理论，与推断统计学联系密切，关注可行学习算法。
-   **发展历程**
    -   **推理期（1950 - 1970s）**：以逻辑推理为重点，如“逻辑理论家”和“通用问题求解”程序，证明了《数学原理》中的定理。2006年卡耐基梅隆大学成立第一个“机器学习系”。
    -   **知识期（1970中期 - 1980s）**：大量专家系统问世，但面临“知识工程瓶颈”。
    -   **学习期**：图灵在1950年提到机器学习可能，包括符号主义学习（如决策树、基于逻辑的学习）、连接主义学习（如基于神经网络的感知机、Adaline，以及深度学习）、统计学习（如支持向量机及核方法）。
-   **近期相关ACM图灵奖**
    -   2011年，Leslie Valiant，“计算学习理论”
    -   2012年，Judea Pear，“图模型学习方法”
    -   2018年，Geoffrey Hinton、Yoshua Bengio、Yann LeCun，“神经网络与深度学习”

### （二）与相关概念关系

-   **模式识别与机器学习**：类似但起源不同，模式识别源自工业界，机器学习来自计算机学科，可视为同一领域的两个方面。
-   **机器学习与人工智能**：机器学习是实现人工智能的途径，也是智能数据分析技术的源泉之一。

### （三）应用现状

-   **在各领域广泛应用**：数据挖掘、计算机视觉、自然语言处理、生物特征识别、搜索引擎、医学诊断、检测信用卡欺诈、证券市场分析、DNA序列测序、语音和手写识别、战略游戏、机器人等。
-   **对多方面产生重要影响**
    -   计算机领域活跃分支，为诸多学科提供技术支撑，如生物信息学。
    -   与生活密切相关，用于天气预报、能源勘探、环境监测、商业营销、搜索引擎、自动驾驶等。
    -   影响政治生活，如2012美国大选。
    -   具有自然科学探索色彩，如SDM模型促进理解“人类如何学习”。

### （四）基本过程

-   **表示（Representation）**：将数据对象特征化，如天气预测中用温度、湿度等特征表示每天，判断好瓜坏瓜用色泽、根蒂、敲声等特征表示瓜。
-   **训练（Training/Learning）**：从数据中学得模型，通过执行学习算法，学习器在训练集上学习潜在规律（假设），目标是使模型具有泛化能力，适用于未知数据。
-   **测试（Testing/Predicting/Inference）**：用学到的模型对新数据样本进行预测，如对新瓜判断好坏。

### （五）基本术语

-   **数据相关**
    -   **示例/样本**：每条记录是对事件或对象的描述。
    -   **属性/特征**：反映对象某方面表现或性质，属性取值为属性值，属性张成属性空间/样本空间/输入空间。
    -   **特征向量**：多个属性组成的向量。
    -   **标记**：关于样本结果的信息，标记集合为标记空间，数据集包括训练集和测试集。
-   **任务相关**
    -   **预测目标**
        -   **分类**：标记为离散值，如二分类（好瓜/坏瓜）、多分类（冬瓜/南瓜/西瓜）。
        -   **回归**：预测连续值，如瓜的成熟度。
        -   **聚类**：无标记信息。
    -   **有无标记信息**
        -   **监督学习**：从标记训练数据推断功能，如分类、回归。
        -   **无监督学习**：在未标记数据中找隐藏结构，如聚类、密度估计。
        -   **半监督学习**：结合监督和无监督学习。

### （六）假设空间与归纳偏好

-   **假设空间**：学得模型对应数据的潜在规律，所有可能假设组成假设空间。
-   **归纳偏好**：学习算法对某种类型假设的偏好，学习算法必须有偏好才能产出认为“正确”的模型，因为存在多条曲线与有限样本训练集一致，且没有免费的午餐定理表明算法性能因问题而异，不能脱离具体问题空谈算法好坏。

### （七）阅读材料

-   **入门读物**
    -   《Pattern Classification》（Duda等著）
    -   《Introduction to Machine Learning》（Alpaydin著）
    -   《Machine Learning: The Art and Science of Algorithms that Make Sense of Data》（Flach著）
-   **进阶读物**
    -   《The Elements of Statistical Learning》（Hastie等著）
    -   《Pattern Recognition and Machine Learning》（Bishop著，适合贝叶斯学习偏好者）
    -   《Understanding Machine Learning》（Shalev - Schwartz和Ben - David著，适合理论偏好者）
-   **其他重要文献**
    -   《机器学习：一种人工智能途径》（Michalski等著）
    -   《人工智能手册》系列（第三卷，Feigenbaum与他人合作编写，对机器学习进行讨论）
    -   国内统计学读物（如陆汝钤的《人工智能（下册）》、李航的《统计学习方法》）
-   **学术会议与期刊**
    -   **机器学习领域**
        -   国际学术会议：国际机器学习会议（ICML）、国际神经信息处理系统会议（NeurIPS）、国际学习理论会议（COLT）、国际表征学习大会（ICLR）等。
        -   区域性会议：欧洲机器学习会议（ECML）、亚洲机器学习会议（ACML）。
        -   国际学术期刊：Journal of Machine Learning Research、Machine Learning。
        -   国内重要活动：中国机器学习大会（CCML，两年一次）、机器学习及其应用研讨会（MLA）。
    -   **人工智能领域**：国际人工智能联合会议（IJCAI）、国际先进人工智能协会年会（AAAI）、Artificial Intelligence、Journal of Artificial Intelligence Research。
    -   **数据领域**：国际数据挖掘与知识发现大会（KDD）、国际数据挖掘会议（ICDM）、ACM Transactions on Knowledge Discovery from Data、Data Mining and Knowledge Discovery。
    -   **计算机视觉领域**：国际计算机视觉与模式识别会议（CVPR）、计算机视觉国际大会（ICCV）、IEEE Transactions on Pattern Analysis and Machine Intelligence（T - PAMI）、International Journal of Computer Vision。
    -   **神经网络领域**：Neural Computation、IEEE Transactions on Neural Networks and Learning Systems。

# 模型评估与选择方法笔记

## 一、引言

### （一）典型机器学习过程中的问题

在典型的机器学习过程中，使用学习算法从训练数据中学习模型，期望模型能对新数据样本有良好的泛化能力，如错误率低、精度高。然而，由于手上没有未知样本，需要通过特定方法评估模型性能，从而引出了模型评估与选择的相关问题。

### （二）关键问题概述

1.  **如何获得测试结果**：涉及评估方法的选择，如留出法、交叉验证法、留一法、自助法等，关键在于如何获取合适的测试集来近似泛化误差。
2.  **如何评估性能优劣**：通过性能度量指标，如回归任务的均方误差、分类任务的错误率与精度、查准率与查全率、F1度量、ROC曲线及AUC等，从不同角度衡量模型泛化能力，但不同度量标准可能导致不同评判结果。
3.  **如何判断实质差别**：考虑到测试性能不等于泛化性能且受多种因素影响，如测试集变化和算法随机性，需借助假设检验来判断学习器性能差异是否具有统计意义，而非直接比较评估方法度量下的大小。

## 二、经验误差与过拟合

### （一）错误率与误差定义

1.  **错误率（error rate）**：分类错误的样本数占总样本数的比例，计算公式为$$E = a / m$$，其中$$a$$为分类错误的样本数，$$m$$为总样本数。
2.  **正确率（accuracy）**：$$1 - a / m$$，与错误率互补。
3.  **误差（error）**
    -   学习器的实际预测输出与样本的真实输出之间的差异。
    -   训练（经验）误差：在训练集上的误差。
    -   测试误差：在测试集上的误差，常将其作为泛化误差的近似，但泛化误差是除训练集外所有样本的误差，希望得到泛化误差小的学习器。

### （二）过拟合与欠拟合现象

1.  **过拟合（overfitting）**
    -   学习能力过于强大，把训练样本学习得“太好”，将训练样本本身的特点当作所有样本的一般性质，导致泛化性能下降。
    -   例如在判断树叶的分类模型中，过拟合模型可能会认为树叶必须有锯齿（仅根据训练样本特点），从而对新样本的分类出现错误。
    -   无法彻底避免，可通过优化目标加正则项、early stop等方法缓解。
2.  **欠拟合（underfitting）**
    -   学习能力低下，对训练样本的一般性质尚未学好。
    -   如在判断树叶时，欠拟合模型可能会认为绿色的都是树叶（没有学习到树叶的关键特征）。
    -   决策树可通过拓展分支、神经网络可通过增加训练轮数来改善欠拟合情况。

## 三、评估方法

### （一）留出法（hold - out）

1.  **数据集划分**
    -   将包含$$m$$个样本的数据集$$D = \{ \left( x_{1} , y_{1} \right) , \left( x_{2} , y_{2} \right) , \cdots , \left( x_{m} , y_{m} \right) \}$$直接拆分成训练集$$S$$和测试集$$T$$，需满足$$D = S \cup T$$且$$S \cap T = \varnothing$$。
    -   为保持数据分布一致性，常通过分层采样划分，如数据集$$D$$包含$$500$$个正例和$$500$$个反例，若划分出含$$70 \%$$样本的训练集$$S$$，则$$S$$应包含$$350$$个正例和$$350$$个反例，测试集$$T$$包含$$150$$个正例和$$150$$个反例。
2.  **优缺点**
    -   优点：简单直接。
    -   缺点：单次使用得到的估计结果不够稳定可靠；训练集和测试集大小会影响评估结果，若$$S$$大$$T$$小则评估结果不够稳定准确，若$$S$$小$$T$$大则$$S$$与$$D$$差别大；常见做法是训练/测试样本比例为$$2 : 1$$到$$4 : 1$$，一般需若干次随机划分、重复实验取平均值。

### （二）交叉验证法（cross validation）

1.  **操作流程**
    -   将数据集$$D$$分层采样划分为$$k$$个大小相似的互斥子集，每次用$$k - 1$$个子集的并集作为训练集，余下的子集作为测试集，最终返回$$k$$个测试结果的均值，$$k$$常取$$10$$。
    -   例如$$10$$折交叉验证，将数据集$$D$$划分为$$D_{1} , D_{2} , \cdots , D_{10}$$，第一次以$$D_{1}$$为测试集，$$D_{2} - D_{10}$$为训练集进行测试，第二次以$$D_{2}$$为测试集，$$D_{1} , D_{3} - D_{10}$$为训练集进行测试，以此类推，共进行$$10$$次测试，最后取均值。
    -   与留出法类似，划分方式存在多种，为减小因样本划分不同引入的差别，$$k$$折交叉验证通常随机使用不同划分重复$$p$$次，最终评估结果是$$p$$次$$k$$折交叉验证结果的均值，如常见的“$$10$$次$$10$$折交叉验证”。
2.  **特点**：能在一定程度上减小样本划分对评估结果的影响，结果相对更可靠，但计算开销比留出法大。

### （三）留一法（Leave - One - Out）

1.  **原理**：当数据集$$D$$包含$$m$$个样本时，令$$k = m$$，则得到留一法。此时$$m$$个样本只有唯一的方式划分为$$m$$个子集，每次用$$m - 1$$个样本训练，剩下$$1$$个样本测试，共进行$$m$$次测试，结果往往比较准确。
2.  **局限性**：计算开销巨大，当数据集较大时，如包含$$1$$百万个样本，则需训练$$1$$百万个模型，难以承受。

### （四）自助法（bootstrapping）

1.  **操作过程**：以自助采样法为基础，对样本量为$$m$$的数据集$$D$$有放回采样$$m$$次得到训练集$$D '$$，用$$D - D '$$做测试集。实际模型与预期模型都使用$$m$$个训练样本，约有$$1 / 3$$的样本没在训练集中出现（$$\lim_{m \rightarrow \infty} \left( 1 - \frac{1}{m} \right)^{m} \rightarrow \frac{1}{e} \approx 0 . 368$$）。
2.  **适用场景与注意事项**：从初始数据集中能产生多个不同训练集，对集成学习有好处，在数据集较小、难以有效划分训练/测试集时很有用；但由于改变了数据集分布可能引入估计偏差，在数据量足够时，留出法和交叉验证法更常用。

### （五）“调参”与最终模型

1.  **参数分类**
    -   算法的参数（超参数）：一般由人工设定。
    -   模型的参数：一般由学习确定。
2.  **调参过程与影响**：调参先产生若干模型，基于某种评估方法选择，参数调得好坏对最终性能有关键影响。算法参数选定后，要用“训练集 + 验证集”重新训练最终模型，需注意训练集、测试集和验证集的区别与作用。

## 四、性能度量

### （一）性能度量的重要性与影响因素

性能度量是衡量模型泛化能力的评价标准，反映任务需求。使用不同性能度量会导致不同评判结果，因此选择合适的性能度量对准确评估模型至关重要。

### （二）不同任务的性能度量指标

1.  **回归任务 - 均方误差（mean squared error）**
    -   计算公式：$$E ( f ; D ) = \frac{1}{m} \sum_{i = 1}^{m} \left( f \left( x_{i} \right) - y_{i} \right)^{2}$$，其中$$f$$为预测函数，$$D$$为训练数据$$\{ \left( x_{1} , y_{1} \right) , \cdots , \left( x_{m} , y_{m} \right) \}$$，$$\hat{y}_{i} = \hat{f} \left( x_{i} \right)$$为预测值，$$y_{i}$$为真实值。
2.  **分类任务**
    -   **错误率与精度**
        -   错误率：$$E ( f ; D ) = \frac{1}{m} \sum_{i = 1}^{m} \mathbb{I} \left( f \left( x_{i} \right) \neq y_{i} \right)$$，$$\mathbb{I}$$为指示函数，当括号内条件成立时值为$$1$$，否则为$$0$$。
        -   精度：$$a c c ( f ; D ) = 1 - E ( f ; D ) = \frac{1}{m} \sum_{i = 1}^{m} \mathbb{I} \left( f \left( x_{i} \right) = y_{i} \right)$$。
    -   **查准率（precision）与查全率（recall）**
        -   在信息检索、Web搜索等场景常用，以二分类为例，通过混淆矩阵计算。混淆矩阵中，$$T P$$（真正例）表示实际为正例且被预测为正例的样本数，$$F N$$（假反例）表示实际为正例但被预测为反例的样本数，$$F P$$（假正例）表示实际为反例但被预测为正例的样本数，$$T N$$（真反例）表示实际为反例且被预测为反例的样本数。查准率$$P = \frac{T P}{T P + F P}$$，查全率$$R = \frac{T P}{T P + F N}$$。
        -   查准率和查全率是一对矛盾度量，一般查准率高时查全率往往偏低，反之亦然。例如在选瓜场景中，若希望选出的瓜中好瓜比例高（偏重查准率），则可能会漏掉不少好瓜（查全率低）；若希望将好瓜尽可能多地选出来（偏重查全率），则查准率可能会较低。
    -   **P - R曲线与平衡点**
        -   学习器为测试样本产生概率预测值，与阈值比较分类。根据概率预测结果对测试样本排序，若更重视“查准率”，可选择靠前位置截断；若更重视“查全率”，可选择靠后位置截断。按此顺序逐个把样本作为正例进行预测，可得到查准率 - 查全率曲线（P - R曲线）。平衡点是曲线上“查准率 = 查全率”时的取值，可用来度量P - R曲线有交叉的分类器性能高低，但对于曲线交叉情况，平衡点判断性能优劣有局限性。
    -   **F1度量与**$$F_{\beta}$$**度量**
        -   $$F 1$$度量：$$F 1 = \frac{2 \times P \times R}{P + R} = \frac{2 \times T P}{样 例 总 数 + T P - T N}$$，比P - R曲线平衡点更常用。
        -   $$F_{\beta}$$度量：$$F_{\beta} = \frac{\left( 1 + \beta^{2} \right) \times P \times R}{\left( \beta^{2} \times P \right) + R}$$，$$\beta = 1$$时为标准$$F 1$$；$$\beta > 1$$时偏重查全率（如逃犯信息检索）；$$\beta < 1$$时偏重查准率（如商品推荐系统）。
    -   **ROC曲线与AUC**
        -   ROC曲线：以假正例率（$$F P R = \frac{F P}{T N + F P}$$）为横轴，真正例率（$$T P R = \frac{T P}{T P + F N}$$）为纵轴。根据预测结果对样例排序，逐个为正例进行预测，若为真正例，对应标记点坐标为$$\left( x , y + \frac{1}{m^{+}} \right)$$（$$m^{+}$$为正例个数）；若为假正例，对应标记点坐标为$$\left( x + \frac{1}{m^{-}} , y \right)$$（$$m^{-}$$为负例个数），然后用线段连接相邻点。对角线对应于“随机猜测”模型，点$$( 0 , 1 )$$对应于将所有正例排在所有反例之前的“理想模型”。
        -   AUC（曲线下面积）：可估算为$$A U C = \frac{1}{2} \sum_{i = 1}^{m - 1} \left( x_{i + 1} - x_{i} \right) \cdot \left( y_{i} + y_{i + 1} \right)$$，衡量了样本预测的排序质量。若某个学习器的ROC曲线被另一个学习器的曲线“包住”，则后者性能优于前者；若曲线交叉，可根据AUC值比较，AUC值越大，模型性能越好。

## 五、误差分析 - 偏差与方差分解

### （一）偏差与方差的定义

1.  **期望输出与真实标记的差别 - 偏差（bias）**
    -   对测试样本$$x$$，令$$y_{D}$$为$$x$$在数据集中的标记，$$y$$为$$x$$的真实标记，$$f ( x ; D )$$为训练集$$D$$上学得模型$$f$$在$$x$$上的预测输出。学习算法的期望预期为$$\overline{f} ( x ) = \mathbb{E}_{D} \left\lbrack f ( x ; D ) \right\rbrack$$，偏差$$b i a s^{2} ( x ) = \left( \overline{f} ( x ) - y \right)^{2}$$，反映算法本身的拟合能力。
2.  **同样大小训练集变动导致的性能变化 - 方差（variance）**
    -   使用样本数目相同的不同训练集产生的方差为$$v a r ( x ) = \mathbb{E}_{D} \left\lbrack \left( f ( x ; D ) - \overline{f} ( x ) \right)^{2} \right\rbrack$$，刻画数据扰动对模型性能的影响。
3.  **噪声（noise）**
    -   噪声为$$\varepsilon^{2} = \mathbb{E}_{D} \left\lbrack \left( y_{D} - y \right)^{2} \right\rbrack$$，表示训练样本的标记与真实标记的区别，刻画学习问题本身的难度，是当前任务上任何学习算法所能达到的期望泛化误差下界。

### （二）泛化误差的分解公式与意义

泛化误差可分解为偏差、方差与噪声之和，即$$E ( f ; D ) = b i a s^{2} ( x ) + v a r ( x ) + \varepsilon^{2}$$。此分解有助于理解泛化性能由学习算法能力、数据充分性及学习任务难度共同决定。

### （三）偏差 - 方差窘境

偏差与方差存在冲突，称为偏差 - 方差窘境。在训练不足时，偏差主导泛化错误率，因为学习器拟合能力不强，训练数据扰动对拟合能力影响小；随着训练程度加深，方差逐渐主导泛化错误率；训练充足后，若学习器拟合能力过强，训练数据轻微扰动都会导致显著变化，可能学到训练数据自身非全局特性而发生过拟合。理解此关系有助于在模型训练中找到平衡，优化模型性能。

# 线性模型笔记

## 一、线性模型概述

### （一）主要内容

包括线性回归、二分类任务（对数几率回归、线性判别分析）、多分类任务（一对一、一对其余、多对多策略）以及类别不平衡问题。线性模型形式简单、易于建模，是非线性模型的基础，具有可解释性，其参数能直观表达各属性在预测中的重要性。

### （二）机器学习任务步骤

1.  **确定目标函数类型**：明确想要找到的函数类型，如线性函数等。
2.  **定义函数集合**：例如线性模型中定义$$f ( x ) = w^{T} x + b$$这样的函数形式，确定参数$$w$$和$$b$$的取值范围等。
3.  **评估函数优劣（性能度量）**：如回归任务中常用均方误差等指标衡量函数对数据的拟合程度；分类任务中用错误率、精度、查准率、查全率、F1度量、ROC曲线及AUC等评估分类效果。
4.  **选择最优函数**：根据性能度量结果，从定义的函数集合中挑选出表现最佳的函数。

## 二、线性回归

### （一）目的与数据集

1.  **目的**：学得一个线性模型以尽可能准确地预测实值输出标记。
2.  **数据集**：给定数据集$$D = \{ \left( x_{1} , y_{1} \right) , \left( x_{2} , y_{2} \right) , \cdots , \left( x_{m} , y_{m} \right) \}$$，其中$$x_{i} = \left( x_{i 1} ; x_{i 2} ; \cdots ; x_{i d} \right)$$，$$y_{i} \mathbb{\in R}$$。

### （二）模型形式与误差计算

1.  **模型形式**：$$f ( x ) = w^{T} x + b$$，其中$$w = \left( w_{1} ; w_{2} ; \cdots ; w_{d} \right)$$，$$x = \left( x_{1} ; x_{2} ; \cdots ; x_{d} \right)$$。
2.  **回归误差**：$$e_{i} = y_{i} - \left( w^{T} x_{i} + b \right)$$。

### （三）参数估计 - 最小二乘法

1.  **原理**：最小化均方误差，即$$\left( w^{*} , b^{*} \right) = \underset{( w , b )}{a r g m i n} \sum_{i = 1}^{m} \left( y_{i} - w^{T} x_{i} - b \right)^{2}$$。
2.  **求解过程**
    -   分别对$$w$$和$$b$$求导，可得$$\frac{\partial E_{( w , b )}}{\partial w} = 2 \left( w \sum_{i = 1}^{m} {x_{i}^{2} -} \sum_{i = 1}^{m} \left( y_{i} - b \right) x_{i} \right)$$，$$\frac{\partial E_{( w , b )}}{\partial b} = 2 \left( m b - \sum_{i = 1}^{m} \left( y_{i} - w^{T} x_{i} \right) \right)$$。
    -   得到闭式解$$w = \frac{\sum_{i = 1}^{m} y_{i} \left( x_{i} - \overline{x} \right)}{\sum_{i = 1}^{m} x_{i}^{2} - \frac{1}{m} \left( \sum_{i = 1}^{m} x_{i} \right)^{2}}$$（其中$$\overline{x} = \frac{1}{m} \sum_{i = 1}^{m} x_{i}$$），$$b = \frac{1}{m} \sum_{i = 1}^{m} \left( y_{i} - w^{T} x_{i} \right)$$。

### （四）多元线性回归

1.  **目标**：$$f \left( x_{i} \right) = w^{T} x_{i} + b$$使得$$f \left( x_{i} \right) \simeq y_{i}$$。
2.  **数据集表示**：把$$w$$和$$b$$吸收入向量形式$$\hat{w} = ( w ; b )$$，数据集可表示为$$X = \begin{pmatrix}x_{11} & x_{12} & \cdots & x_{1 d} & 1 \\ x_{21} & x_{22} & \cdots & x_{2 d} & 1 \\ \vdots & \vdots & \ddots & \vdots & \vdots \\ x_{m 1} & x_{m 2} & \cdots & x_{m d} & 1\end{pmatrix} = \begin{pmatrix}x_{1}^{T} & 1 \\ x_{2}^{T} & 1 \\ \vdots & \vdots \\ x_{m}^{T} & 1\end{pmatrix}$$，$$y = \left( y_{1} ; y_{2} ; \cdots ; y_{m} \right)$$。
3.  **最小二乘法求解**：$$\hat{w}^{*} = \underset{\hat{w}}{a r g m i n} \left( y - X \hat{w} \right)^{T} \left( y - X \hat{w} \right)$$，令$$E_{\hat{w}} = \left( y - X \hat{w} \right)^{T} \left( y - X \hat{w} \right)$$，对$$\hat{w}$$求导得到$$\frac{\partial E_{\hat{w}}}{\partial \hat{w}} = 2 X^{T} \left( X \hat{w} - y \right)$$，令上式为零可得$$\hat{w}$$最优解的闭式解。当$$X^{T} X$$是满秩矩阵或正定矩阵时，$$\hat{w}^{*} = \left( X^{T} X \right)^{- 1} X^{T} y$$；当$$X^{T} X$$不是满秩矩阵时，可根据归纳偏好选择解或引入正则化。

### （五）线性回归中的线性含义

线性并不指对输入变量的线性，而是指对参数空间的线性。可以对输入进行非线性变换后再进行线性组合，通用非线性化方法有核学习方法。

### （六）线性模型的变化 - 对数线性回归与广义线性模型

1.  **对数线性回归**：输出标记的对数为线性模型逼近的目标，即$$\ln y = w^{T} x + b$$，$$y = e^{w^{T} x + b}$$。
2.  **广义线性模型**：一般形式为$$y = g^{- 1} \left( w^{T} x + b \right)$$，其中$$g ( \cdot )$$称为联系函数（单调可微函数），对数线性回归是$$g ( \cdot ) = \ln ( \cdot )$$时广义线性模型的特例。

## 三、二分类任务

### （一）对数几率回归

1.  **预测值与输出标记关系**：预测值$$z = w^{T} x + b$$，输出标记$$y \in \{ 0 , 1 \}$$。
2.  **联系函数选择**
    -   **单位阶跃函数**：$$y = \begin{cases}0 , & z < 0 \\ 0 . 5 , & z = 0 \\ 1 , & z > 0\end{cases}$$，不连续，实际应用中常用其替代函数。
    -   **对数几率函数**：$$y = \frac{1}{1 + e^{- z}}$$，单调可微、任意阶可导。
3.  **对数几率回归模型**：运用对数几率函数，得到$$\ln \frac{y}{1 - y} = w^{T} x + b$$，无需事先假设数据分布，不仅能预测类别，还可得到类别近似概率预测，可直接应用现有数值优化算法求取最优解（求解目标函数是任意阶可导的凸函数）。
4.  **参数求解 - 极大似然法**
    -   给定数据集$$\{ \left( x_{i} , y_{i} \right) \}_{i = 1}^{m}$$，将$$y$$视为类后验概率估计$$p ( y = 1 | x )$$，可得$$p ( y = 1 | x ) = \frac{e^{w^{T} x + b}}{1 + e^{w^{T} x + b}}$$，$$p ( y = 0 | x ) = \frac{1}{1 + e^{w^{T} x + b}}$$。
    -   极大似然法最大化样本属于其真实标记的概率，即最大化对数似然函数$$\mathcal{l} ( w , b ) = \sum_{i = 1}^{m} \ln p \left( y_{i} | x_{i} ; w , b \right)$$，转化为最小化负对数似然函数$$\mathcal{l} ( \beta ) = \sum_{i = 1}^{m} \left( - y_{i} \beta^{T} \hat{x}_{i} + \ln \left( 1 + e^{\beta^{T} \hat{x}_{i}} \right) \right)$$（令$$\beta = ( w ; b )$$，$$\hat{x} = ( x ; 1 )$$）。
    -   求解可使用牛顿法，其第$$t + 1$$轮迭代解的更新公式为$$\beta^{t + 1} = \beta^{t} - \left( \frac{\partial^{2} \mathcal{l} ( \beta )}{\partial \beta \partial \beta^{T}} \right)^{- 1} \frac{\partial \mathcal{l} ( \beta )}{\partial \beta}$$，其中关于$$\beta$$的一阶、二阶导数分别为$$\frac{\partial \mathcal{l} ( \beta )}{\partial \beta} = - \sum_{i = 1}^{m} \hat{x}_{i} \left( y_{i} - p_{1} \left( \hat{x}_{i} ; \beta \right) \right)$$，$$\frac{\partial^{2} \mathcal{l} ( \beta )}{\partial \beta \partial \beta^{T}} = \sum_{i = 1}^{m} \hat{x}_{i} \hat{x}_{i}^{T} p_{1} \left( \hat{x}_{i} ; \beta \right) \left( 1 - p_{1} \left( \hat{x}_{i} ; \beta \right) \right)$$。

### （二）线性判别分析（LDA）

1.  **基本思想**：将样例投影到一条直线上，使同类样例的投影点尽可能接近，异类样例的投影点尽可能远离，可视为一种监督降维技术。
2.  **相关变量定义**
    -   第$$i$$类示例的集合$$X_{i}$$。
    -   第$$i$$类示例的均值向量$$\mu_{i}$$。
    -   第$$i$$类示例的协方差矩阵$$\sum_{i}^{}$$。
    -   两类样本的中心在直线上的投影：$$w^{T} \mu_{0}$$和$$w^{T} \mu_{1}$$。
    -   两类样本的协方差：$$w^{T} \sum_{0}^{} w$$和$$w^{T} \sum_{1}^{} w$$。
3.  **最大化目标**：$$J = \frac{\| w^{T} \mu_{0} - w^{T} \mu_{1} \|_{2}^{2}}{w^{T} \sum_{0}^{} w + w^{T} \sum_{1}^{} w} = \frac{w^{T} \left( \mu_{0} - \mu_{1} \right) \left( \mu_{0} - \mu_{1} \right)^{T} w}{w^{T} \left( \sum_{0} + \sum_{1} \right) w}$$，运用拉格朗日乘子法，令$$w^{T} S_{w} w = 1$$（其中类内散度矩阵$$S_{w} = \sum_{0}^{} + \sum_{1}^{} = \sum_{x \in X_{0}}^{} \left( x - \mu_{0} \right) \left( x - \mu_{0} \right)^{T} + \sum_{x \in X_{1}}^{} \left( x - \mu_{1} \right) \left( x - \mu_{1} \right)^{T}$$，类间散度矩阵$$S_{b} = \left( \mu_{0} - \mu_{1} \right) \left( \mu_{0} - \mu_{1} \right)^{T}$$），得到$$S_{b} w = \lambda S_{w} w$$，求解可得$$w = S_{w}^{- 1} \left( \mu_{0} - \mu_{1} \right)$$。
4.  **适用条件**：两类数据同先验、满足高斯分布且协方差相等时，LDA达到最优分类。

## 四、多分类任务

### （一）LDA推广到多分类

1.  **相关矩阵定义**
    -   全局散度矩阵$$S_{t} = S_{b} + S_{w} = \sum_{i = 1}^{m} \left( x_{i} - \mu \right) \left( x_{i} - \mu \right)^{T}$$。
    -   类内散度矩阵$$S_{w} = \sum_{i = 1}^{N} S_{w_{i}}$$（其中$$S_{w_{i}} = \sum_{x \in X_{i}}^{} \left( x - \mu_{i} \right) \left( x - \mu_{i} \right)^{T}$$），进而可得$$S_{b} = S_{t} - S_{w} = \sum_{i = 1}^{N} m_{i} \left( \mu_{i} - \mu \right) \left( \mu_{i} - \mu \right)^{T}$$。
2.  **优化目标**：$$\max_{W} \frac{\text{tr} \left( W^{T} S_{b} W \right)}{\text{tr} \left( W^{T} S_{w} W \right)}$$（其中$$W \in \mathbb{R}^{d \times ( N - 1 )}$$），其闭式解是$$S_{w}^{- 1} S_{b}$$的$$N - 1$$个最大广义特征值所对应的特征向量组成的矩阵。多分类LDA将样本投影到$$N - 1$$维空间（$$N - 1$$通常远小于数据原有的属性数），因此也是一种监督降维技术。

### （二）多分类学习方法

1.  **基于二分类学习器的策略**
    -   **一对一（OvO）**
        -   **拆分阶段**：$$N$$个类别两两配对，产生$$N ( N - 1 ) / 2$$个二类任务，为每个任务训练一个分类器。
        -   **测试阶段**：新样本提交给所有分类器预测，得到$$N ( N - 1 ) / 2$$个分类结果，通过投票产生最终分类结果（被预测最多的类别为最终类别）。
    -   **一对其余（OvR）**
        -   **拆分阶段**：某一类作为正例，其他类作为反例，共产生$$N$$个二类任务，训练$$N$$个分类器。
        -   **测试阶段**：新样本提交给所有分类器预测，得到$$N$$个分类结果，选择置信度最大的类别作为最终类别。
    -   **多对多（MvM） - 纠错输出码（ECOC）**
        -   **编码阶段**：对$$N$$个类别做$$M$$次划分，每次划分将一部分类别划为正类，一部分划为反类，形成$$M$$个训练集，训练出$$M$$个分类器。
        -   **解码阶段**：测试样本交给$$M$$个分类器预测，这些预测标记组成一个编码，计算该编码与长度为$$M$$的各个类别的编码的距离（如海明距离、欧式距离等），距离最小的类别为最终类别。
2.  **两种策略比较**
    -   **OvO**：训练$$N ( N - 1 ) / 2$$个分类器，存储开销和测试时间大，但训练只用两个类的样例，训练时间短。
    -   **OvR**：训练$$N$$个分类器，存储开销和测试时间小，但训练用到全部训练样例，训练时间长。预测性能取决于具体数据分布，多数情况下两者差不多。

## 五、类别不平衡问题

### （一）问题描述

不同类别训练样例数相差很大（正类为小类）的情况，定义正负类比例。

### （二）解决策略

1.  **再缩放**：通过调整阈值来平衡类别，如$$\frac{y}{1 - y} > \frac{m^{+}}{m^{-}}$$（类别平衡正例预测$$\frac{y}{1 - y} > 1$$）。
2.  **欠采样（undersampling）**：去除一些反例使正反例数目接近（如EasyEnsemble算法）。
3.  **过采样（oversampling）**：增加一些正例使正反例数目接近（如SMOTE算法）。
4.  **阈值移动（threshold - moving）**：根据类别不平衡情况调整分类阈值。

## 六、总结

### （一）各模型优化目标

1.  **线性回归**：最小二乘法最小化均方误差。
2.  **对数几率回归**：最大化样本分布似然。
3.  **线性判别分析**：投影空间内最小（大）化类内（间）散度。

### （二）参数优化方法

1.  **线性回归（最小二乘法）**：利用线性代数知识求解。
2.  **对数几率回归**：采用凸优化中的梯度下降法、牛顿法等。
3.  **线性判别分析**：运用矩阵论知识，通过广义瑞利商求解。

线性模型在机器学习中具有重要地位，通过不同的方法和策略可以应用于回归、二分类、多分类以及处理类别不平衡等多种任务场景，为解决实际问题提供了有效的手段。在实际应用中，需要根据具体问题的特点和数据情况选择合适的线性模型及相关方法进行建模和分析。

# 支持向量机学习笔记

## 一、支持向量机概述

### （一）基本思想

1.  线性模型，在样本空间中寻找超平面将不同类别样本分开。
2.  选择“正中间”的超平面，对训练样本扰动容忍性好、鲁棒性高、泛化能力强。

### （二）面临问题及解决方法

1.  线性不可分问题：将样本映射到高维特征空间使线性可分（核函数）。
2.  难以确定合适核函数及避免过拟合：引入软间隔概念。

### （三）相关概念

1.  超平面方程：$$w^{T} x + b = 0$$。
2.  间隔：$$r = \frac{\left| w^{T} x + b \right|}{\| w \|}$$，支持向量是使等号成立的样本点。

### （四）支持向量机基本型

$$\begin{aligned}\underset{w , b}{a r g m i n} & \frac{1}{2} \| w \|^{2} \\ s . t . & y_{i} \left( w^{\top} x_{i} + b \right) \geq 1 , i = 1 , 2 , . . . , m .\end{aligned}$$，即寻找$$w$$和$$b$$使间隔最大。

### （五）解的稀疏性

1.  最终模型仅与支持向量有关，大部分训练样本无需保留。
2.  对于任意训练样本，$$\alpha_{i} = 0$$或$$y_{i} f \left( x_{i} \right) = 1$$，$$\alpha_{i} = 0$$时样本不影响最终模型，$$\alpha_{i} > 0$$时样本为支持向量。

### （六）偏移项确定

通过支持向量确定，$$b = \frac{1}{| S |} \sum_{s \in S}^{} \left( y_{s} - \sum_{i \in S} \alpha_{i} y_{i} x_{i}^{T} x_{s} \right)$$，$$S$$为支持向量下标集。

### （七）求解方法 - SMO

1.  基本思路：不断选取一对变量$$\alpha_{i}$$和$$\alpha_{j}$$，固定其他参数，求解对偶问题更新这两个变量，直至收敛。
2.  仅考虑$$\alpha_{i}$$和$$\alpha_{j}$$时，对偶问题约束变为$$\alpha_{i} y_{i} + \alpha_{j} y_{j} = - \sum_{k \neq i , j}^{} \alpha_{k} y_{k} , \alpha_{i} \geq 0 , \alpha_{j} \geq 0$$，可转化为单变量二次规划问题，有闭式解。

## 二、对偶问题

### （一）引入拉格朗日乘子得到拉格朗日函数

$$L ( w , b , \alpha ) = \frac{1}{2} \| w \|^{2} - \sum_{i = 1}^{m} \alpha_{i} \left( y_{i} \left( w^{\top} x_{i} + b \right) - 1 \right)$$，$$\alpha_{i} \geq 0$$。

### （二）求偏导为零得到

$$w = \sum_{i = 1}^{m} \alpha_{i} y_{i} x_{i}$$，$$\sum_{i = 1}^{m} \alpha_{i} y_{i} = 0$$。

### （三）回代得到对偶问题

$$\begin{array}{r}m a x_{\alpha} - \frac{1}{2} \sum_{i = 1}^{m} {\sum_{j = 1}^{m} \alpha_{i}} \alpha_{j} y_{i} y_{j} x_{i}^{\top} x_{j} + \sum_{i = 1}^{m} \alpha_{i} \\ s . t . \sum_{i = 1}^{m} \alpha_{i} y_{i} = 0 , \alpha_{i} \geq 0 , i = 1 , 2 , . . . , m .\end{array}$$，其为二次规划问题。

### （四）KKT条件

1.  $$- f_{i} \left( w_{*} \right) \leq 0 , i = 1 , . . . , m$$（原始可行）。
2.  $$- h_{i} \left( w_{*} \right) = 0 , i = 1 , . . . , p$$（原始可行）。
3.  $$- \nabla f \left( w_{*} \right) + \sum_{i = 1}^{m} \lambda_{i}^{*} \nabla f_{i} \left( w_{*} \right) + \sum_{i = 1}^{p} \nu_{i}^{*} \nabla h_{i} \left( w_{*} \right) = 0$$（原始）。
4.  $$- \lambda^{*} \succeq 0$$（对偶可行）。
5.  对$$f_{i} \left( w_{*} \right) = 0$$，$$i = 1 , . . . , m$$（互补松弛条件）。

### （五）对偶问题解的性质

1.  若$$\alpha_{i} = 0$$，样本不影响最终模型；若$$\alpha_{i} > 0$$，样本为支持向量且$$y_{i} f \left( x_{i} \right) = 1$$。
2.  训练完成后，最终模型仅与支持向量有关。

## 三、核函数

### （一）引入原因

解决线性不可分问题，将样本映射到高维特征空间使线性可分。

### （二）定义

设样本$$x$$映射后的向量为$$\phi ( x )$$，划分超平面为$$f ( x ) = w^{\top} \phi ( x ) + b$$，核函数$$\kappa \left( x_{i} , x_{j} \right) = \phi \left( x_{i} \right)^{\top} \phi \left( x_{j} \right)$$，不显式设计核映射，而是设计核函数。

### （三）Mercer定理

对称函数对应的核矩阵半正定，则可作为核函数使用。

### （四）常用核函数

1.  线性核：$$\kappa \left( x_{i} , x_{j} \right) = x_{i}^{T} x_{j}$$。
2.  多项式核：$$\kappa \left( x_{i} , x_{j} \right) = \left( x_{i}^{T} x_{j} \right)^{d}$$，$$d \geq 1$$为多项式次数。
3.  高斯核：$$\kappa \left( x_{i} , x_{j} \right) = \exp \left( - \frac{\| x_{i} - x_{j} \|^{2}}{2 \sigma^{2}} \right)$$，$$\sigma > 0$$为带宽。
4.  拉普拉斯核：$$\kappa \left( x_{i} , x_{j} \right) = \exp \left( - \frac{\| x_{i} - x_{j} \|}{\sigma} \right)$$，$$\sigma > 0$$。
5.  Sigmoid核：$$\kappa \left( x_{i} , x_{j} \right) = \tan h \left( \beta x_{i}^{T} x_{j} + \theta \right)$$，$$\tanh$$为双曲正切函数，$$\beta > 0$$，$$\theta < 0$$。

### （五）核函数性质

1.  若$$\kappa_{1}$$和$$\kappa_{2}$$为核函数，$$\gamma_{1}$$、$$\gamma_{2}$$为正数，$$\gamma_{1} \kappa_{1} + \gamma_{2} \kappa_{2}$$也是核函数。
2.  若$$\kappa_{1}$$和$$\kappa_{2}$$为核函数，核函数直积$$\kappa_{1} \otimes \kappa_{2} ( x , z ) = \kappa_{1} ( x , z ) \kappa_{2} ( x , z )$$也是核函数。
3.  若$$\kappa_{1}$$为核函数，对任意函数$$g ( x )$$，$$\kappa ( x , z ) = g ( x ) \kappa_{1} ( x , z ) g ( z )$$也是核函数。

## 四、软间隔与正则化

### （一）软间隔概念引入原因

1.  现实中难确定合适核函数使训练样本在特征空间线性可分。
2.  线性可分结果难断定是否过拟合。

### （二）软间隔基本想法

允许支持向量机在一些样本上不满足约束$$y_{i} \left( w^{T} x + b \right) \geq 1$$，最大化间隔同时让不满足约束的样本尽可能少。

### （三）损失函数

1.  **0/1损失函数**：$$l_{0 / 1} = \begin{cases}1 & z < 0 \\ 0 & o t h e r w i s e\end{cases}$$，非凸、非连续，不易优化。
2.  **替代损失函数**
    -   指数损失：$$e ( z ) = \exp ( - z )$$。
    -   对率损失：$$l_{h i n g e} ( z ) = \max ( 0 , 1 - z )$$，$$\log \left( 1 + \exp ( - z ) \right)$$，数学性质较好，是0/1损失函数上界。

### （四）软间隔支持向量机原始问题

$$\begin{array}{r}m i n_{w , b} \frac{1}{2} \| w \|^{2} + C \sum_{i = 1}^{m} m a x \left( 0 , 1 - y_{i} \left( w^{\top} \phi \left( x_{i} \right) + b \right) \right)\end{array}$$（hinge损失），引入松弛变量$$\xi_{i}$$，$$\begin{array}{r}m i n_{w , b} \frac{1}{2} \| w \|^{2} + C \sum_{i = 1}^{m} \xi_{i} \\ s . t . \xi_{i} \geq 1 - y_{i} \left( w^{T} x_{i} + b \right) , \xi_{i} \geq 0 , i = 1 , \cdots , m .\end{array}$$。

### （五）软间隔支持向量机对偶问题

$$\begin{array}{r}m i n_{\alpha} \frac{1}{2} \sum_{i = 1}^{m} {\sum_{j = 1}^{m} \alpha_{i}} \alpha_{j} y_{i} y_{j} \phi \left( x_{i} \right)^{\top} \phi \left( x_{j} \right) - \sum_{i = 1}^{m} \alpha_{i} \\ s . t . \sum_{i = 1}^{m} \alpha_{i} y_{i} = 0 , 0 \leq \alpha_{i} \leq C , i = 1 , 2 , . . . , m .\end{array}$$，根据KKT条件，最终模型仅与支持向量有关，保持解的稀疏性。

### （六）KKT条件及样本情况分析

1.  KKT条件：$$\begin{cases}\alpha_{i} \geq 0 , \mu_{i} \geq 0 , \\ y_{i} f \left( x_{i} \right) - 1 + \xi_{i} \geq 0 , \\ \alpha_{i} \left( y_{i} f \left( x_{i} \right) - 1 + \xi_{i} \right) = 0 , \\ \xi_{i} \geq 0 , \mu_{i} \xi_{i} = 0 .\end{cases}$$。
2.  样本情况
    -   若$$\alpha_{i} = 0$$，样本对分类面无影响。
    -   若$$\alpha_{i} > 0$$，则$$y_{i} f \left( x_{i} \right) = 1 - \xi_{i}$$，为支持向量。
        -   若$$\alpha_{i} < C$$，$$\mu_{i} > 0$$，$$\xi_{i} = 0$$，在间隔边界上。
        -   若$$\alpha_{i} = C$$，$$0 < \xi_{i} < 1$$，分类正确，在间隔边界与超平面之间。
        -   若$$\alpha_{i} = C$$，$$\xi_{i} = 1$$，在超平面上。
        -   若$$\alpha_{i} = C$$，$$\xi_{i} > 1$$，分类错误。

### （七）正则化

支持向量机学习模型更一般形式$$m i n_{f} \Omega ( f ) + C \sum_{i = 1}^{m} l \left( f \left( x_{i} \right) , y_{i} \right)$$，$$\Omega ( f )$$为结构风险，描述模型性质，$$l \left( f \left( x_{i} \right) , y_{i} \right)$$为经验风险，描述模型与训练数据契合程度，通过替换可得到其他学习模型，如对数几率回归、最小绝对收缩选择算子等。

## 五、支持向量回归

### （一）特点

允许模型输出和实际输出间存在$$\epsilon$$的偏差。

### （二）损失函数

落入中间$$2 \epsilon$$间隔带的样本不计算损失，使模型获得稀疏性。

1.  最小二乘损失函数$$\mathcal{l} ( z ) = z^{2}$$。
2.  支持向量回归损失函数$$\mathcal{l}_{\epsilon} ( z ) = \begin{cases}0 & i f | z | \leq \epsilon \\ | z | - \epsilon & o t h e r w i s e\end{cases}$$。

### （三）形式化

1.  **原始问题** $$\begin{array}{r}m i n_{w , b , \xi_{i} , \hat{\xi}_{i}} \frac{1}{2} \| w \|^{2} + C \sum_{i = 1}^{m} \left( \xi_{i} + \hat{\xi}_{i} \right) \\ s . t . y_{i} - w^{\top} \phi \left( x_{i} \right) - b \leq \epsilon + \xi_{i} , y_{i} - w^{\top} \phi \left( x_{i} \right) - b \geq - \epsilon - \hat{\xi}_{i} , \\ \xi_{i} \geq 0 , \hat{\xi}_{i} \geq 0 , i = 1 , 2 , . . . , m .\end{array}$$。
2.  **对偶问题** $$\begin{aligned}m i n_{\alpha , \hat{\alpha}} & \frac{1}{2} \sum_{i = 1}^{m} {\sum_{j = 1}^{m} \left( \alpha_{i} - \hat{\alpha}_{i} \right)} \left( \alpha_{j} - \hat{\alpha}_{j} \right) \kappa \left( x_{i} , x_{j} \right) + \sum_{i = 1}^{m} \left( \alpha_{i} \left( \epsilon - y_{i} \right) + \hat{\alpha}_{i} \left( \epsilon + y_{i} \right) \right) \\ s . t . & \sum_{i = 1}^{m} \left( \alpha_{i} - \hat{\alpha}_{i} \right) = 0 , \\  & 0 \leq \alpha_{i} \leq C , 0 \leq \hat{\alpha}_{i} \leq C .\end{aligned}$$。

### （四）KKT条件及变量取值分析

1.  KKT条件：$$\begin{cases}\alpha_{i} \left( f \left( x_{i} \right) - y_{i} - \epsilon - \xi_{i} \right) = 0 , \\ \hat{\alpha}_{i} \left( y_{i} - f \left( x_{i} \right) - \epsilon - \hat{\xi}_{i} \right) = 0 , \\ \alpha_{i} \hat{\alpha}_{i} = 0 , \xi_{i} \hat{\xi}_{i} = 0 , \\ \left( C - \alpha_{i} \right) \xi_{i} = 0 , \left( C - \hat{\alpha}_{i} \right) \hat{\xi}_{i} = 0 .\end{cases}$$。
2.  仅当样本$$\left( x_{i} , y_{i} \right)$$不落入$$\epsilon$$间隔带中，相应的$$\alpha_{i}$$和$$\hat{\alpha}_{i}$$才能取非零值，且$$\alpha_{i}$$和$$\hat{\alpha}_{i}$$中至少有一个为零。

## 六、核方法

### （一）表示定理

1.  支持向量机$$f ( x ) = w^{\top} \phi ( x ) + b = \sum_{i = 1}^{m} \alpha_{i} y_{i} \kappa \left( x_{i} , x \right) + b$$，支持向量回归$$f ( x ) = w^{\top} \phi ( x ) + b = \sum_{i = 1}^{m} \left( \hat{\alpha}_{i} - \alpha_{i} \right) y_{i} \kappa \left( x_{i} , x \right) + b$$。
2.  对于任意单调增函数$$\Omega$$和非负损失函数$$l$$，优化问题$$m i n_{h \mathbb{\in H}} F ( h ) = \Omega \left( \| h \|_{\mathbb{H}} \right) + l \left( h \left( x_{1} \right) , . . . , h \left( x_{m} \right) \right)$$的解总可以写为$$h^{*} = \sum_{i = 1}^{m} \alpha_{i} \kappa \left( \cdot , x_{i} \right)$$。

### （二）核线性判别分析

先将样本映射到高维特征空间，然后在此特征空间中做线性判别分析，如核SVM、核LDA、核PCA等，核LDA为$$m a x_{w} J ( w ) = \frac{w^{\top} S_{b}^{\phi} w}{w^{\top} S_{w}^{\phi} w}$$，$$m a x_{\alpha} J ( \alpha ) = \frac{\alpha^{\top} M \alpha}{\alpha^{\top} N \alpha}$$。

### （三）相关软件包

1.  LIBSVM：http://www.csie.ntu.edu.tw/\~cjlin/libsvm/。
2.  LIBLINEAR：http://www.csie.ntu.edu.tw/\~cjlin/liblinear/。
3.  SVMlight、SVMperf、SVMstruct：http://

# 决策树学习笔记

## 一、决策树概述

### （一）定义

基于树结构进行预测，可用于分类和回归任务。内部节点表示属性判断，分支表示判断结果输出，叶节点表示分类结果。

### （二）目的

产生泛化能力强（处理未见示例能力强）的决策树。

### （三）面临问题

1.  如何选择属性作为根节点及后继节点。
2.  何时停止划分得到目标值。

### （四）关键在于选择属性

选择使决策树分支节点“纯度”越来越高的属性，常用属性划分方法有信息增益、增益率、基尼指数。

### （五）算法流程

1.  生成结点node。
2.  若样本全属同一类别，标记node为该类别叶结点并返回。
3.  若属性集为空或样本在属性上取值相同，标记node为叶结点（类别为样本数最多的类）并返回。
4.  从属性集中选最优划分属性$$a_{*}$$。
5.  为node生成每个分支，对每个取值$$a_{*}^{v}$$，若对应样本子集$$D_{v}$$为空，标记分支结点为叶结点（类别为D中样本最多的类）并返回，否则以TreeGenerate($$D$$, $$A - \{ a_{*} \} )$$为分枝结点。

## 二、纯度

### （一）定义

反映目标变量的混乱（分歧）程度，纯度越高，混乱程度越小。

### （二）示例

集合1（6次打篮球）纯度\>集合2（4次打篮球、2次不打）\>集合3（3次打篮球、3次不打）。

### （三）信息熵

1.  **定义**：度量样本集合纯度的常用指标，$$E n t ( D ) = - \sum_{k = 1}^{| Y |} p_{k} l o g_{2} p_{k}$$，$$E n t ( D )$$值越小，纯度越高。反映信息不确定性，不确定性越大，信息熵越高，纯度越低。计算约定$$p = 0$$时，$$p l o g_{2} p = 0$$，$$E n t ( D )$$最小值为0，最大值为$$l o g_{2} | Y |$$。
2.  **计算示例**
    -   集合1（5次打篮球、1次不打）：$$E n t r o p y ( t ) = - ( 1 / 6 ) l o g_{2} ( 1 / 6 ) - ( 5 / 6 ) l o g_{2} ( 5 / 6 ) = 0 . 65$$。
    -   集合2（3次打篮球、3次不打）：$$E n t r o p y ( t ) = - ( 3 / 6 ) l o g_{2} ( 3 / 6 ) - ( 3 / 6 ) l o g_{2} ( 3 / 6 ) = 1$$。

## 三、划分选择

### （一）信息增益

1.  **定义**：离散属性$$a$$有$$V$$个取值，用$$a$$划分产生$$V$$个分支结点，第$$v$$个分支结点包含属性$$a$$取值为$$a^{v}$$的样本$$D^{v}$$，信息增益$$G a i n ( D , a ) = E n t ( D ) - \sum_{v = 1}^{V} \frac{\left| D^{v} \right|}{| D |} E n t \left( D^{v} \right)$$，信息增益越大，用属性$$a$$划分纯度提升越大。
2.  **计算步骤**
    -   计算根结点信息熵$$E n t ( D )$$。
    -   对每个属性，计算其每个取值对应的子集信息熵$$E n t \left( D^{v} \right)$$。
    -   计算属性的信息增益$$G a i n ( D , a )$$。
3.  **实例**：以西瓜数据集为例，计算各属性信息增益，如色泽的信息增益为0.109，纹理的信息增益为0.381等，纹理信息增益最大被选为划分属性。
4.  **问题**：若把“编号”作为候选划分属性，其信息增益远大于其他属性，但这样的决策树不具泛化能力，信息增益准则对可取值数目较多的属性有偏好。

### （二）增益率

1.  **定义**：为减少信息增益偏好的不利影响，增益率$$G a i n_{r} a t i o ( D , a ) = \frac{G a i n ( D , a )}{I V ( a )}$$，其中$$I V ( a ) = - \sum_{v = 1}^{V} \frac{\left| D^{v} \right|}{| D |} l o g_{2} \frac{\left| D^{v} \right|}{| D |}$$为属性$$a$$的“固有值”，属性$$a$$取值数目越多，$$I V ( a )$$值通常越大。增益率准则对可取值数目较少的属性有所偏好。
2.  **C4.5算法策略**：先从候选划分属性中找出信息增益高于平均水平的属性，再从中选取增益率最高的。

### （三）基尼指数

1.  **定义**：基尼值$$G i n i ( D ) = \sum_{k = 1}^{| Y |} {\sum_{k ' \neq k}^{} p_{k}} p_{k '} = 1 - \sum_{k = 1}^{| Y |} p_{k}^{2}$$，基尼指数$$G i n i_{i} n d e x ( D , a ) = \sum_{v = 1}^{V} \frac{\left| D^{v} \right|}{| D |} G i n i \left( D^{v} \right)$$，应选择使划分后基尼指数最小的属性作为最优划分属性，即$$a_{*} = \underset{a \in A}{a r g m i n} G i n i_{i} n d e x ( D , a )$$。
2.  **CART算法使用**：CART采用基尼指数选择划分属性。

## 四、剪枝处理

### （一）目的

对付“过拟合”，避免决策分支过多导致把训练集自身特点当作一般性质。

### （二）基本策略

1.  **预剪枝**：在决策树生成过程中，对每个结点划分前估计，若当前结点划分不能提升决策树泛化性能，则停止划分并记为叶结点，类别标记为训练样例数最多的类别。优点是降低过拟合风险、减少训练和测试时间开销；缺点是有欠拟合风险，可能禁止一些后续划分能提升性能的分支展开。
2.  **后剪枝**：先从训练集生成完整决策树，自底向上考察非叶结点，若将子树替换为叶结点能提升泛化性能，则替换。优点是保留更多分支，欠拟合风险小，泛化性能优于预剪枝决策树；缺点是训练时间开销大，需自底向上考察所有非叶结点。

### （三）判断决策树泛化性能提升的方法

留出法，预留一部分数据用作“验证集”进行性能评估。

### （四）剪枝处理实例

1.  **预剪枝**：对西瓜数据集，基于信息增益准则选“脐部”划分训练集，计算划分前后验证集精度，如结点1不划分验证集精度为42.9%，划分后为71.4%，则划分；对后续结点继续判断，最终得到仅有一层划分的决策树。
2.  **后剪枝**：先生成完整决策树，验证集精度为42.9%，然后自底向上考察结点，如考虑结点6，替换为叶结点后验证集精度提高至57.1%，则剪枝；对其他结点依次判断，最终得到后剪枝后的决策树。

## 五、连续与缺失值

### （一）连续值处理

1.  **连续属性离散化（二分法）**
    -   第一步：假定连续属性$$a$$在样本集$$D$$上有$$n$$个不同取值，从小到大排列为$$a^{1} , a^{2} , \ldots , a^{n}$$，基于划分点$$t$$将$$D$$分为子集$$D_{t}^{-}$$（属性$$a$$取值不大于$$t$$）和$$D_{t}^{+}$$（属性$$a$$取值大于$$t$$），候选划分点集合$$T_{a} = \{ \frac{a^{i} + a^{i + 1}}{2} | 1 \leq i \leq n - 1 \}$$。
    -   第二步：采用离散属性值方法考察划分点，选取最优划分点进行样本集合划分，$$G a i n ( D , a ) = \max_{t \in T_{a}} G a i n ( D , a , t ) = \max_{t \in T_{a}} E n t ( D ) - \sum_{\lambda \in \{ - , + \}}^{} \frac{\left| D_{t}^{\lambda}  \right|}{| D |} E n t \left( D_{t}^{\lambda}  \right)$$。
2.  **连续值处理实例**：以西瓜数据集的密度和含糖率属性为例，计算其候选划分点集合及信息增益，密度信息增益为0.262（对应划分点0.381），含糖率信息增益为0.349（对应划分点0.126），纹理信息增益最大被选为划分属性，且连续属性可在后代结点继续作为划分属性。

### （二）缺失值处理

1.  **问题提出**：不完整样本（属性值缺失），仅使用无缺失样本学习浪费数据信息，需解决在属性缺失情况下划分属性选择及样本划分问题。
2.  **解决方法**
    -   对于划分属性选择，令$$\overset{\sim}{D}$$表示$$D$$中属性$$a$$无缺失值的样本子集，$$\overset{\sim}{D^{v}}$$表示$$\overset{\sim}{D}$$中属性$$a$$取值为$$a^{v}$$的样本子集，$$\overset{\sim}{D_{k}}$$表示$$\overset{\sim}{D}$$中属于第$$k$$类的样本子集，为样本$$x$$赋予权重$$w_{x}$$，定义无缺失值样本所占比例$$\rho = \frac{\sum_{x \in \overset{\sim}{D}}^{} w_{x}}{\sum_{x \in D}^{} w_{x}}$$，无缺失值样本中第$$k$$类所占比例$$\overset{\sim}{p_{k}} = \frac{\sum_{x \in \overset{\sim}{D_{k}}}^{} w_{x}}{\sum_{x \in \overset{\sim}{D}}^{} w_{x}}$$，无缺失值样本中在属性$$a$$上取值$$a^{v}$$的样本所占比例$$\overset{\sim}{r_{v}} = \frac{\sum_{x \in \overset{\sim}{D^{v}}}^{} w_{x}}{\sum_{x \in \overset{\sim}{D}}^{} w_{x}}$$，则$$G a i n ( D , a ) = \rho \times G a i n \left( \overset{\sim}{D} , a \right) = \rho \times \left( E n t \left( \overset{\sim}{D} \right) - \sum_{v = 1}^{V} \overset{\sim}{r_{v}} E n t \left( \overset{\sim}{D^{v}} \right) \right)$$。
    -   对于样本划分，若样本$$x$$在划分属性$$a$$上取值已知，划入对应子结点且权值不变；若取值未知，同时划入所有子结点，在与属性值$$a^{v}$$对应的子结点中权值调整为$$\overset{\sim}{r_{v}} \cdot w_{x}$$。
3.  **缺失值处理实例**：以西瓜数据集为例，计算属性“色泽”在有缺失值情况下的信息增益，如色泽在无缺失值子集$$\overset{\sim}{D}$$上信息增益为0.306，在样本集$$D$$上信息增益为0.252，类似计算其他属性信息增益，纹理信息增益最大进入纹理分支，在纹理属性上出现缺失值时样本8和10按规则进入各分支并调整权值。

## 六、多变量决策树

### （一）单变量决策树分类边界

轴平行，由若干与坐标轴平行的分段组成。

### （二）多变量决策树

1.  非叶节点是对属性的线性组合，形如$$\sum_{i = 1}^{d} w_{i} a_{i} = t$$的线性分类器，$$w_{i}$$是属性$$a_{i}$$的权值，$$w_{i}$$和$$t$$可在结点所含样本集和属性集上学得。
2.  相比单变量决策树，多变量决策树在某些情况下可得到更简洁有效的分类边界。

## 七、决策树相关软件包

ID3、C4.5、C5.0、J48等，可通过相关网站获取使用。

# 集成学习笔记

## 1. 集成学习概述

### 1.1定义

通过构建并结合多个学习器来提升性能，也被称为多分类器系统、基于委员会的学习。

### 1.2个体与集成

-   **个体学习器类型**：同质（基学习器相同）、异质（组件学习器不同）。
-   **集成思想**：把好坏不等的东西掺在一起，结果通常比最坏的好、比最好的坏；要获得比最好单一学习器更好性能，集成个体应“好而不同”（兼具准确性和多样性）。
-   **简单分析**
    -   二分类问题中，假设基分类器错误率为$$\epsilon$$，集成通过简单投票法结合$$T$$个分类器，若超过半数基分类器正确则分类正确，在基分类器错误率相互独立假设下，由Hoeffding不等式可得集成错误率$$P \left( H ( x ) \neq f ( x ) \right) = \sum_{k = 0}^{\lfloor T / 2 \rfloor} \begin{pmatrix}T \\ k\end{pmatrix} ( 1 - \epsilon )^{k} \epsilon^{T - k} \leq e x p \left( - \frac{1}{2} T ( 1 - 2 \epsilon )^{2} \right)$$，显示随着集成分类器数目增加，错误率指数级下降趋向于$$0$$，但现实中基学习器误差相互独立假设不成立，个体学习器“准确性”和“多样性”存在冲突。

### 1.3集成学习分类

-   **序列化方法（Boosting）**：个体学习器存在强依赖关系，必须串行生成。
-   **并行化方法**
    -   **Bagging**：个体学习器间不存在强依赖关系，可同时生成。
    -   **随机森林**：Bagging扩展变种，以决策树为基学习器，增加了属性扰动。

## 2. Bagging与随机森林

### 2.1 Bagging

-   **基本思想**：并行构建$$T$$个分类器，并行训练，通过自助采样法（bootstrap sampling）产生训练集，对每个样本有放回随机抽取，最终综合所有分类器结果（平均或投票）得到预测结果。
-   **算法特点**
    -   时间复杂度低，与直接使用基学习器复杂度同阶。
    -   可使用包外估计，约$$63 . 2 \%$$样本用于训练，剩余约$$36 . 8 \%$$样本可作验证集估计泛化性能。
-   **实验效果**：从偏差 - 方差角度降低方差，在不剪枝决策树、神经网络等易受样本影响学习器上效果好。

### 2.2随机森林

-   **采样随机性**：对基决策树每个结点，先从属性集合随机选包含$$k$$（通常$$\log_{2} d$$）个属性子集，再从中选最优属性划分。
-   **实验效果**：收敛性与Bagging类似，随基分类器数目增加通常收敛到更低泛化误差；基学习器多样性高于Bagging，通过样本扰动和属性扰动提升性能。

## 3. Boosting

### 3.1 Boosting算法

-   **基本思想**：个体学习器存在强依赖关系，串行生成，每次调整训练数据样本分布，根据基学习器表现对训练样本加权，使先前做错样本后续受更多关注，最终将基学习器加权结合。
-   **代表算法**：AdaBoost。

### 3.2 AdaBoost算法

-   **算法流程**
    1.  初始化训练集样本分布$$\mathcal{D}_{1} ( x ) = 1 / m$$。
    2.  从初始训练集依分布$$\mathcal{D}_{t}$$训练出学习器$$h_{t}$$，计算错误率$$\epsilon_{t} = P_{x \sim \mathcal{D}_{t}} \left( h_{t} ( x ) \neq f ( x ) \right)$$，若$$\epsilon_{t} > 0 . 5$$则停止。
    3.  计算$$h_{t}$$权重$$\alpha_{t} = \frac{1}{2} \ln \left( \frac{1 - \epsilon_{t}}{\epsilon_{t}} \right)$$。
    4.  更新样本分布$$\mathcal{D}_{t + 1} ( x ) = \frac{\mathcal{D}_{t} ( x )}{Z_{t}} \times \begin{cases}\exp \left( - \alpha_{t} \right) , & i f h_{t} ( x ) = f ( x ) \\ \exp \left( \alpha_{t} \right) , & i f h_{t} ( x ) \neq f ( x )\end{cases}$$（$$Z_{t}$$为归一化因子）。
    5.  重复2 - 4步直至基学习器数目达到$$T$$，最终输出$$H ( x ) = s i g n \left( \sum_{t = 1}^{T} \alpha_{t} h_{t} ( x ) \right)$$。
-   **核心思想**：由弱分类器逐步强化成强分类器，通过改变样本权重，让分类器更关注错误分类点。
-   **实验效果**：从偏差 - 方差角度降低偏差，可对泛化性能弱的学习器构造强集成。
-   **特点**
    1.  **优点**：不易过拟合；可使用不同学习算法构建弱分类器；充分考虑每个分类器权重；参数少。
    2.  **缺点**：迭代次数难设定（可交叉验证确定）；对异常样本敏感；训练耗时。

## 4. 结合策略

### 4.1平均法

-   **简单平均法**：$$H ( x ) = \frac{1}{T} \sum_{i = 1}^{T} h_{i} ( x )$$，适用于个体学习器性能相近情况。
-   **加权平均法**：$$H ( x ) = \sum_{i = 1}^{T} w_{i} h_{i} ( x )$$（$$w_{i} \geq 0$$），个体学习器性能相差较大时适用，加权平均法可视为集成学习研究基本出发点，各种结合方法可看成其变种或特例，但未必优于简单平均法。

### 4.2投票法

-   **绝对多数投票法**：得票过半数确定标记，$$H ( x ) = \begin{cases}c_{j} & i f \sum_{i = 1}^{T} h_{i}^{j} ( x ) > \frac{1}{2} \sum_{k = 1}^{l} {\sum_{i = 1}^{T} h_{i}^{k}} ( x ) \\ r e j e c t i o n & o t h e r w i s e\end{cases}$$。
-   **相对多数投票法**：得票最多确定标记，$$H ( x ) = c_{a r g m a x} \sum_{i = 1}^{T} h_{i}^{j} ( x )$$。
-   **加权投票法**：$$H ( x ) = c_{a r g m a x} \sum_{i = 1}^{T} w_{i} h_{i}^{j} ( x )$$。

### 4.3学习法（以Stacking为例）

-   **算法流程**
    1.  用不同学习算法（同质或异质）训练$$T$$个第一层学习器$$h_{t}$$。
    2.  用每个学习器对数据集$$D$$中样本预测，生成新数据集$$D '$$。
    3.  用第二层学习算法$$\mathfrak{L}$$在$$D '$$上训练得到最终学习器$$h '$$，输出$$H ( x ) = h ' \left( h_{1} ( x ) , \ldots , h_{T} ( x ) \right)$$。
-   **效果**：降低偏差，可训练$$T + 1$$个模型（K - cross validation时训练$$k T + 1$$个模型）。

## 5. 多样性

### 5.1误差 - 分歧分解

-   **定义**
    -   学习器$$h_{i}$$分歧$$A \left( h_{i} | x \right) = \left( h_{i} ( x ) - H ( x ) \right)^{2}$$，集成分歧$$\overline{A} ( h | x ) = \sum_{i = 1}^{T} w_{i} A \left( h_{i} | x \right) = \sum_{i = 1}^{T} w_{i} \left( h_{i} ( x ) - H ( x ) \right)^{2}$$。
    -   个体学习器$$h_{i}$$和集成$$H$$平方误差分别为$$E \left( h_{i} | x \right) = \left( f ( x ) - h_{i} ( x ) \right)^{2}$$、$$E ( H | x ) = \left( f ( x ) - H ( x ) \right)^{2}$$。
-   **公式推导**：$$\overline{A} ( h | x ) = \sum_{i = 1}^{T} w_{i} E \left( h_{i} | x \right) - E ( H | x )$$，在全样本上有$$E = \overline{E} - \overline{A}$$（$$E$$为集成泛化误差，$$\overline{E} = \sum_{i = 1}^{T} w_{i} E_{i}$$为个体学习器泛化误差加权均值，$$\overline{A} = \sum_{i = 1}^{T} w_{i} A_{i}$$为个体学习器加权分歧值），表明个体学习器精确性高且多样性大时集成效果好，但现实中难以直接优化$$\bar{E} - \bar{A}$$。

### 5.2多样性度量

-   **常见度量方法**
    -   不合度量$$d i s_{i j} = \frac{b + c}{m}$$（$$a + b + c + d = m$$，基于分类器预测结果联立表）。
    -   相关系数$$\rho_{i j} = \frac{a d - b c}{\sqrt{( a + b ) ( a + c ) ( c + d ) ( b + d )}}$$。
    -   $$Q -$$统计量$$Q_{i j} = \frac{a d - b c}{a d + b c}$$（$$\left| Q_{i j} \right| \geq \left| \rho_{i j} \right|$$）。
    -   $$\kappa -$$统计量$$\kappa = \frac{p_{1} - p_{2}}{1 - p_{2}}$$（$$p_{1} = \frac{a + d}{m}$$，$$p_{2} = \frac{( a + b ) ( a + c ) + ( c + d ) ( b + d )}{m^{2}}$$）。
-   $$\kappa -$$**误差图**：数据点云位置越高个体分类器准确性越低，越靠右个体学习器多样性越小。

### 5.3多样性增强

-   **常见方法**
    -   数据样本扰动（如Bagging自助采样法、Adaboost序列采样），对不稳定基学习器（如决策树、神经网络）有效。
    -   输入属性扰动（如随机森林属性采样）。
    -   输出表示扰动（翻转法、输出调剂法、ECOC法）。
    -   算法参数扰动（负相关法、不同多样性增强机制同时使用）。

# 聚类分析学习笔记

聚类是无监督学习中的重要任务，旨在将数据集中的样本划分为若干个不相交的子集（簇），以揭示数据的内在分布结构，在个性化服务、社交网络、数据分析等领域广泛应用。以下是聚类分析的学习笔记，涵盖聚类任务、性能度量、距离计算、原型聚类、密度聚类和层次聚类等方面。

## 1. 聚类任务

### 1.1形式化描述

样本集$$D = \{ x_{1} , x_{2} , \cdots , x_{m} \}$$，每个样本$$x_{i} = \left( x_{i 1} ; x_{i 2} ; \cdots ; x_{i n} \right)$$是$$n$$维特征向量。聚类算法将$$D$$划分为$$k$$个不相交簇$$\{ C_{l} | l = 1 , 2 , \cdots , k \}$$，满足$$C_{l '} \cap_{l ' \neq l} C_{l} = \varnothing$$且$$D = \cup_{l = 1}^{k} C_{l}$$。样本$$x_{j}$$的簇标记为$$\lambda_{j} \in \{ 1 , 2 , \cdots , k \}$$，聚类结果用簇标记向量$$\lambda = \{ \lambda_{1} ; \lambda_{2} ; \cdots ; \lambda_{m} \}$$表示。

### 1.2应用

-   个性化服务：根据用户行为和特征进行聚类，提供个性化推荐。
-   社交网络：分析用户关系和兴趣，发现社交圈子和社区结构。
-   数据分析：对文档、图像等数据进行聚类，帮助理解数据主题和结构。

### 1.3基本问题

-   性能度量：评估聚类结果好坏，作为优化目标。
-   距离计算：考虑有序、无序属性，选择合适距离度量方式。

## 2. 性能度量

### 2.1聚类性能度量概述

也称为聚类“有效性指标”，直观上希望“物以类聚”，即同一簇样本尽可能相似（簇内相似度高），不同簇样本尽可能不同（簇间相似度低）。度量方式分为外部指标和内部指标。

### 2.2外部指标

将聚类结果与参考模型进行比较，通过计算特定样本对数量来定义指标，如FM指数、Rand指数、Jaccard系数等，取值范围在[0,1]，值越大表示聚类结果与参考模型越吻合。

### 2.3内部指标

直接考察聚类结果，定义了簇内样本间平均距离、最远距离、簇间最近样本距离、簇中心点间距离等概念，用于计算DB指数、Dunn指数等指标，DB指数越小越好，Dunn指数越大越好。

## 3. 距离计算

### 3.1距离度量性质

包括非负性、同一性、对称性和直递性。常用距离度量方式有闵可夫斯基距离（当$$p = 2$$为欧氏距离，$$p = 1$$为曼哈顿距离）。

### 3.2属性类型与距离计算

-   连续属性：定义域上有无穷多个取值。
-   离散属性：定义域上有有限个取值，又分为有序属性（如取值{1,2,3}，值之间有远近关系）和无序属性（如{飞机,火车,轮船}，不能直接在属性值上计算）。

### 3.3处理无序属性和混合属性的距离度量

-   Value Difference Metric (VDM)：用于处理无序属性，计算两个离散值之间的距离。
-   MinkovDMp：处理混合属性，结合了连续属性的距离计算和无序属性的VDM距离。

### 3.4加权距离和距离度量学习

当样本中不同属性重要性不同时使用加权距离。距离不一定提前定义好，可根据数据样本动态学习（距离度量学习），且用于度量相似性的“距离”未必满足所有基本性质（非度量距离）。

## 4. 原型聚类

### 4.1原型聚类概述

也称“基于原型的聚类”，假设聚类结构能通过一组原型刻画。算法通常先初始化原型，再迭代更新求解。包括k-means、学习向量量化（LVQ）、高斯混合聚类（GMM）等算法。

### 4.2 k-means算法

-   **目标函数**：针对聚类所得簇划分$$C = \{ C_{1} , C_{2} , \cdots , C_{k} \}$$最小化平方误差$$E = \sum_{i = 1}^{k} {\sum_{x \in C_{j}}^{} \|} x - \mu_{i} \|_{2}^{2}$$，其中$$\mu_{i}$$是簇$$C_{i}$$的均值向量，$$E$$值刻画簇内样本围绕均值向量的紧密程度，$$E$$越小簇内样本相似度越高。
-   **算法流程**：随机初始化均值向量，重复更新簇划分和计算均值向量，直到均值向量不再更新。
-   **优缺点**：优点是原理简单、易实现、可解释性强；缺点是$$k$$值难确定，效果依赖聚类中心初始化（可能陷入局部最优），对噪音和异常点敏感，对非凸数据集或类别规模差异大的数据效果不好。

### 4.3学习向量量化（LVQ）

假设数据样本带类别标记，利用监督信息辅助聚类。目标是学得一组$$n$$维原型向量，每个原型向量代表一个聚类簇。算法根据样本与原型向量的距离及类别标记更新原型向量，使同类样本靠近相应原型向量，异类样本远离。

### 4.4高斯混合聚类

采用概率模型（高斯分布）表达聚类原型。样本生成过程由高斯混合分布给出，先根据混合系数选择高斯混合成分，再从所选成分中采样生成样本。聚类时根据样本由各高斯混合成分生成的后验概率确定簇标记，通过最大化（对数）似然估计模型参数（均值向量、协方差矩阵、混合系数），常用EM算法求解，包括E步计算后验概率和M步更新模型参数，不断重复直到满足停止条件。

## 5. 密度聚类

### 5.1密度聚类概述

也称“基于密度的聚类”，假设聚类结构通过样本分布紧密程度确定，从样本密度角度考察可连接性，基于可连接样本扩展聚类簇。核心思想是将每个密集区域当作一个聚类簇，以DBSCAN算法为代表。

### 5.2 DBSCAN算法

-   **基本概念**
    -   $$\epsilon$$邻域：样本$$x_{j}$$的$$\epsilon$$邻域包含$$D$$中与$$x_{j}$$距离不大于$$\epsilon$$的样本。
    -   核心对象：样本$$x_{j}$$的$$\epsilon$$邻域至少包含$$M i n P t s$$个样本，则$$x_{j}$$为核心对象。
    -   密度直达：若$$x_{j}$$位于核心对象$$x_{i}$$的$$\epsilon$$邻域中，则$$x_{j}$$由$$x_{i}$$密度直达。
    -   密度可达：若存在样本序列，其中相邻样本密度直达，则首尾样本密度可达。
    -   密度相连：若两样本均由另一样本密度可达，则两样本密度相连。
-   **簇定义**：由密度可达关系导出的最大密度相连样本集合。形式化描述为满足连接性（簇内样本密度相连）和最大性（若样本在簇内且由另一样本密度可达，则另一样本也在簇内）的非空样本子集。
-   **算法流程**：遍历样本找出核心对象，以核心对象为出发点，通过密度可达关系生成聚类簇，直到所有核心对象被访问。
-   **优缺点**：能发现任意形状簇，对噪声不敏感；但参数$$\epsilon$$和$$M i n P t s$$难确定，密度不均匀数据集聚类效果可能不好。

## 6. 层次聚类

### 6.1层次聚类概述

试图在不同层次对数据集进行划分，形成树形聚类结构。数据集划分可采用“自底向上”聚合策略或“自顶向下”分拆策略，具有灵活停止、不必事先设定簇个数的优点。

### 6.2 AGNES算法（自底向上层次聚类）

-   **算法流程**：初始化将每个样本点看作一个聚类簇，迭代找出距离最近的两个聚类簇合并，直到达到预设聚类簇个数。聚类簇距离度量方式有最小距离、最大距离和平均距离。
-   **算法伪代码**：包括初始化聚类簇和距离矩阵、合并最近聚类簇、更新距离矩阵等步骤。
-   **聚类效果展示**：通过树状图展示聚类过程和结果，可根据需求选择合适聚类簇数，不同$$k$$值对应不同聚类效果。

### 6.3层次聚类优缺点

-   **优点**：不需要预先指定簇个数，聚类结果以树形结构展示，能提供不同粒度聚类信息，适用于对数据集分布结构了解甚少情况。
-   **缺点**：计算复杂度较高，一旦一个合并或者分裂被执行，就不能再撤销，可能导致聚类结果不好。

聚类分析在数据挖掘、机器学习、模式识别等领域有重要应用。不同聚类算法各有优缺点，实际应用中需根据数据特点、任务需求选择合适算法或组合使用，以达到最佳聚类效果。同时，性能度量指标有助于评估和比较不同算法在特定数据集上的表现，为算法选择和优化提供依据。

# 降维与度量学习

## 一、k近邻学习

### （一）概念

1.  **定义**：k近邻（k-Nearest Neighbor, kNN）学习是一种常用的监督学习方法，确定训练样本及某种距离度量。给定测试样本，找到训练集中距离最近的k个样本。分类问题用“投票法”，回归问题用“平均法”，还可基于距离加权平均或加权投票。
2.  **“懒惰学习”与“急切学习”**
    -   **“懒惰学习”**：如kNN，训练阶段仅保存样本，训练时间开销为零，收到测试样本后再处理。
    -   **“急切学习”**：在训练阶段就对样本进行学习处理。

### （二）分析

1.  **k值影响**：k是重要参数，不同k值分类结果不同。
2.  **距离计算方式影响**：不同距离计算方式，找出的“近邻”有差别，导致分类结果不同。
3.  **1NN二分类错误率**：假设距离计算恰当，最近邻分类器（1NN）在二分类问题上，泛化错误率不超过贝叶斯最优分类器错误率的两倍。

### （三）维数灾难

1.  **问题描述**：训练样本采样密度足够大（密采样）的假设在现实任务中难满足。高维情形下出现数据样本稀疏、距离计算困难等问题，即“维数灾难”。如属性维数为1，$$\delta = 0 . 001$$时，1000个样本点平均分布在归一化后的属性取值范围；属性维数为20，满足密采样条件至少需要$$10^{60}$$个样本。
2.  **解决途径**：降维，将原始高维属性空间转变为低维“子空间”，提高样本密度，使距离计算更容易。数据样本虽高维，但与学习任务相关的可能是低维分布（低维“嵌入”），可进行有效降维。

### （四）线性降维方法

1.  **原理**：通过线性变换将原始高维空间转变为低维空间，新空间中的属性是原空间属性的线性组合。给定d维空间中的样本$$X$$，变换矩阵$$W$$，变换后得到$$d ' \leq d$$维空间中的样本$$Z = W^{T} X$$。$$W$$可视为$$d '$$个d维属性向量，若$$w_{i}$$与$$w_{j} ( i \neq j )$$正交，则为正交变换。
2.  **分类及特点**
    -   **线性有监督**：LDA。
    -   **线性无监督**：PCA。优点是对线性结构分布数据集降维效果好，在压缩、降噪、数据可视化等方面有效，计算简单、易于理解；缺点是对非线性或属性强相关数据集，无法发现内在本质结构。

## 二、主成分分析（PCA）

### （一）背景

美国统计学家斯通1947年研究国民经济，利用1929 - 1938年数据，17个变量经PCA后，用3个新变量取代，精度达97.4%，并根据经济学知识命名，且新变量与实际测量因素相关。

### （二）概念

PCA是简化多变量截面数据表分析的方法，在力保数据信息丢失最少原则下，对高维变量空间降维。将p个指标转变为p个指标的线性组合，得到新指标$$F_{1} , F_{2} , \cdots , F_{k} ( k \leq p )$$，充分反映原指标信息且相互独立。

### （三）几何解释

对正交属性空间中的样本点，用超平面对所有样本恰当表达，基于最近重构性（样本点到超平面距离近）和最大可分性（样本点在超平面上投影能分开），可得到PCA的两种等价推导。

### （四）求解

1.  **优化目标**：基于最近重构性或最大可分性，PCA的优化目标为$$\min_{W} - t r \left( W^{T} X X^{T} W \right)$$，$$s . t . W^{T} W = I$$。通过拉格朗日乘子法，对协方差矩阵$$X X^{T}$$进行特征值分解，取前$$d '$$个特征值对应的特征向量构成$$W$$。
2.  **PCA算法**
    -   输入：样本集$$D = \{ x_{1} , x_{2} , \cdots , x_{m} \}$$，低维空间维数$$d '$$。
    -   过程：对样本中心化；计算协方差矩阵$$X X^{T}$$；对协方差矩阵做特征值分解；取最大的$$d '$$个特征值所对应的特征向量。
    -   输出：投影矩阵$$W^{*} = \left( w_{1} , w_{2} , \cdots , w_{d '} \right)$$。

### （五）性质

1.  每个主成分的系数平方和为1，即$$w_{1 i}^{2} + w_{2 i}^{2} + \cdots + w_{p i}^{2} = 1$$。
2.  主成分相互独立，无重叠信息，即$$C o v \left( F_{i} , F_{j} \right) = 0 , i \neq j$$。
3.  主成分方差依次递减，重要性依次递减，即$$V a r \left( F_{1} \right) \geq V a r \left( F_{2} \right) \geq \cdots \geq V a r \left( F_{p} \right)$$。

### （六）降维维数确定

1.  用户指定或通过交叉验证选取。
2.  从重构角度设置重构阈值$$t$$（如$$t = 95 \%$$），选取使$$\frac{\sum_{i = 1}^{d '} \lambda_{i}}{\sum_{i = 1}^{d} \lambda_{i}} \geq t$$成立的最小值。降维虽损失信息，但可增大样本采样密度，去噪。

### （七）示例

假设有n个学生四门课程成绩，标准化处理后得到样本相关矩阵$$R$$。对其特征值分解，按方差贡献率排序，取前两个主成分（累计方差贡献率大于75%），求出对应特征向量，得到第一、第二主成分表达式。

## 三、核化线性降维

### （一）背景

线性降维方法假设从高维到低维空间的函数映射是线性的，但现实任务中常需非线性映射找低维嵌入，如PCA无法表达Helix曲线流形，真实数据有用信息不能由线性特征表示（如多姿态人脸姿态信息）。

### （二）核化主成分分析（KPCA）

1.  基于核技巧对线性降维方法“核化”。在高维特征空间中投影，求解$$\left( \sum_{i = 1}^{m} \phi \left( x_{i} \right) \phi \left( x_{i} \right)^{T} \right) W = \lambda W$$，一般引入核函数$$\kappa \left( x_{i} , x_{j} \right) = \phi \left( x_{i} \right)^{T} \phi \left( x_{j} \right)$$，将问题转化为核矩阵$$K$$的特征值分解问题$$K \alpha^{j} = \lambda_{j} \alpha^{j}$$。
2.  新样本投影后的坐标$$z_{j} = \sum_{i = 1}^{m} \alpha_{i}^{j} \kappa \left( x_{i} , x \right)$$，KPCA计算开销较大，因需对所有样本求和。

## 四、流形学习

### （一）流形概念

1.  观察到的数据是低维流形映射到高维空间的，高维数据存在维度冗余，实际用较低维度可唯一表示。流形是线性子空间的非线性推广，拓扑学角度局部区域线性，与低维欧式空间拓扑同胚，可用欧氏距离计算。

### （二）流形学习概念

流形学习是借鉴拓扑流形概念的非线性降维方法，用于从高维采样数据恢复低维流形结构，可能是人类认知自然行为方式，如人类视觉记忆以流形形式存储，人类有捕获流形结构能力。

### （三）等度量映射（Isomap）

1.  低维流形嵌入高维空间后，高维空间直线距离在低维嵌入流形上不可达，低维嵌入流形两点间本真距离是测地线距离（流形上连接两点的最短曲线，如球面上大圆弧）。
2.  计算测地距离：利用流形局部与欧氏空间同胚性质，找每个点近邻点建近邻连接图，近邻点距离用欧式距离，非近邻点距离设为无穷大，测地线距离计算转为近邻连接图上最短路径问题，可用Dijkstra或Floyd算法。得到距离后用多维缩放方法获样本点低维坐标。

### （四）局部线性嵌入（LLE）

1.  试图保持邻域内线性关系，显式利用局部线性假设，保持局部领域几何结构（重构权重），权重对样本集几何变换具不变性。先为样本找近邻下标集合，计算线性重构系数，在低维空间保持系数不变，通过优化问题求解低维空间坐标。

### （五）Isomap和LLE算法流程

1.  **Isomap算法**
    -   输入：样本集$$D$$，近邻参数$$k$$，低维空间维数$$d '$$。
    -   过程：确定每个样本的$$k$$近邻，设置近邻点距离为欧氏距离、其他点为无穷大，调用最短路径算法计算样本点间距离，将距离作为MDS算法输入，返回MDS算法输出。
    -   输出：样本集在低维空间的投影。
2.  **LLE算法**
    -   输入：样本集$$D$$，近邻参数$$k$$，低维空间维数$$d '$$。
    -   过程：确定每个样本的$$k$$近邻，求重构系数$$w_{i j}$$，构建矩阵$$M$$并特征值分解，返回最小$$d '$$个特征值对应的特征向量。
    -   输出：样本集在低维空间的投影。

## 五、度量学习

### （一）研究动机

机器学习中降维目的是找合适低维空间使学习性能更好，实质是寻找合适距离度量，可尝试学习合适距离度量。

### （二）距离度量表达形式

1.  平方欧氏距离$$d i s t_{e d}^{2} \left( x_{i} , x_{j} \right) = \| x_{i} - x_{j} \|_{2}^{2} = \sum_{k = 1}^{d} d i s t_{i j , k}^{2}$$，引入属性权重$$w$$得到加权平方欧氏距离$$d i s t_{w e d}^{2} \left( x_{i} , x_{j} \right) = \left( x_{i} - x_{j} \right)^{T} W \left( x_{i} - x_{j} \right)$$（$$W = d i a g ( w )$$），但实际属性可能不正交相关，用普通半正定对称矩阵$$M$$得到马氏距离$$d i s t_{m a h}^{2} \left( x_{i} , x_{j} \right) = \left( x_{i} - x_{j} \right)^{T} M \left( x_{i} - x_{j} \right)$$，$$M$$为度量矩阵，度量学习是对$$M$$学习。

### （三）近邻成分分析（NCA）

1.  判别用概率投票法，$$x_{j}$$对$$x_{i}$$分类影响概率$$p_{i j} = \frac{\exp \left( - \| x_{i} - x_{j} \|_{M}^{2}  \right)}{\sum_{l}^{} e x p \left( - \| x_{i} - x_{l} \|_{M}^{2}  \right)}$$，$$x_{i}$$的留一法正确率$$p_{i} = \sum_{j \in \Omega_{i}}^{} p_{i j}$$，优化目标$$\min_{P} 1 - \sum_{i = 1}^{m} {\sum_{j \in \Omega_{i}}^{} \frac{\exp \left( - \| P^{T} x_{i} - P^{T} x_{j} \|_{2}^{2}  \right)}{\sum_{l}^{} e x p \left( - \| P^{T} x_{i} - P^{T} x_{l} \|_{2}^{2}  \right)}}$$，求解得最大化近邻分类器LOO正确率的距离度量矩阵$$M$$。

### （四）度量学习与降维

不同度量学习方法获合适度量矩阵$$M$$，若$$M$$低秩，可分解得正交基，衍生降维矩阵$$P$$用于降维。

# 半监督学习笔记

## 1. 背景

### 1.1信息爆炸时代的数据特点

-   数据量巨大：IBM估计每天产生2.5 quintillion（$$2 . 5 \times 10^{18}$$）bytes数据。
-   有标记数据昂贵稀少，无标记数据廉价海量。如在判断“Covid - 19阳性”“Covid - 19阴性”、辨别鸟的种类等场景中，获取有标记数据成本高，而未标记数据容易获取。

### 1.2相关学习概念对比

-   **主动学习**：引入额外专家知识，通过与外界交互将部分未标记样本变为有标记样本。
-   **（纯）半监督学习**：利用有标记样本和无标记样本训练模型预测待测数据。
-   **直推学习**：利用有标记数据和无标记数据训练模型，预测未标记数据。

### 1.3未标记样本的效用与假设

-   **效用**：若未标记样本与有标记样本从同样数据源独立同分布采样而来，其包含的数据分布信息对建立模型有益。
-   **假设**：包括聚类假设（假设数据存在簇结构，同一簇样本属于同一类别）和流形假设（假设数据分布在流形结构上，邻近样本具有相似输出值，流形假设可看作聚类假设推广）。

## 2. 半监督学习方法

### 2.1半监督SVM（S3VM）

-   **基本思想**：试图找到能将两类有标记样本分开且穿过数据低密度区域的划分超平面，基于“低密度分隔”假设，是聚类假设在线性超平面划分后的推广。
-   **TSVM学习目标**：给定有标记样本集$$D_{l}$$和无标记样本集$$D_{u}$$，为$$D_{u}$$中的样本给出预测标记$$\hat{y}$$，使得$$m i n_{w , b , \hat{y} , \xi} \frac{1}{2} \| w \|_{2}^{2} + C_{l} \sum_{i = 1}^{l} \xi_{i} + C_{u} \sum_{i = l + 1}^{m} \xi_{i}$$，同时满足$$y_{i} \left( w^{\top} x_{i} + b \right) \geq 1 - \xi_{i} , i = 1 , . . . , l$$，$$\hat{y}_{i} \left( w^{\top} x_{i} + b \right) \geq 1 - \xi_{i} , i = l + 1 , . . . , m$$，$$\xi_{i} \geq 0 , i = 1 , . . . , m$$。
-   **算法流程**：
    -   用$$D_{l}$$训练一个SVM，对$$D_{u}$$中样本预测得到伪标记$$\hat{y}$$。
    -   初始化$$C_{u} \ll C_{l}$$，迭代求解优化目标，逐渐增大$$C_{u}$$。迭代中，若发现两个未标记样本为异类且可能错误分类，则交换样本标记重新求解。
-   **问题与改进**：未标记样本标记指派及调整可能出现类别不平衡问题，可将优化目标中$$C_{u}$$项拆分，初始化时令$$C_{u}^{+} = \frac{u_{-}}{u_{+}} C_{u}^{-}$$（$$u_{+}$$与$$u_{-}$$为基于伪标记当作正、反例的未标记样本数）。搜寻标记指派出错样本对调整计算开销大，研究重点是设计高效优化求解策略，如基于图核函数梯度下降的Laplacian SVM、基于标记均值估计的meanS3VM等。

### 2.2基于分歧的方法

-   **协同训练**：
    -   **多视图数据**：假设数据有两个“充分”且“条件独立”视图，每个视图属性集足以描述样本且相互独立。
    -   **算法流程**：从无标记样本集$$D_{u}$$抽取缓冲池$$D_{s}$$，在两个视图上用有标记样本训练基学习器$$h_{1}$$和$$h_{2}$$，用$$h_{1}$$和$$h_{2}$$给$$D_{s}$$中样本生成伪标记，挑选置信度最高的正、反例扩充有标记数据集，再从$$D_{u}$$中抽取样本加入$$D_{s}$$，重复训练直至$$h_{1}$$和$$h_{2}$$不再改变。
    -   **特点与变体**：理论上若视图条件独立，可提升弱分类器泛化性能，但实际中条件独立性难满足，不过在更弱条件下仍可有效提升性能。变体算法可在单视图数据上使用，通过不同学习算法、数据采样或参数设置产生不同学习器，利用学习器之间分歧相互提供伪标记样本提高泛化性能。

### 2.3半监督聚类

-   **利用监督信息类型**：
    -   **“必连”与“勿连”约束**：如约束k均值算法在聚类过程中确保“必连”关系集合与“勿连”关系集合中的约束满足，否则返回错误提示。
    -   **少量有标记样本**：约束种子k均值算法将少量有标记样本作为“种子”初始化k均值算法的聚类中心，且在迭代更新过程中不改变种子样本簇隶属关系。

## 3. 总结

半监督学习在有标记数据稀少、无标记数据丰富的情况下，通过合理利用未标记样本，基于不同假设和方法，如半监督SVM的低密度分隔假设、基于分歧方法中学习器的分歧利用、半监督聚类中的监督信息利用等，能够提高模型的学习效果和泛化能力，在实际应用中具有重要意义。

# 贝叶斯分类器学习笔记

## 1. 先验概率

### 定义

-   事情未发生时，根据以往经验判断事情发生的概率，反映经验知识。例如鲈鱼与鲑鱼分类问题中，用$$w$$标记类别，$$w = w_{1}$$表示鲈鱼，$$w = w_{2}$$表示鲑鱼，$$P \left( w_{1} \right)$$和$$P \left( w_{2} \right)$$分别为下一条鱼是鲈鱼和鲑鱼的先验概率，且$$P \left( w_{1} \right) + P \left( w_{2} \right) = 1$$。

### 简单判决准则

-   当$$P \left( w_{1} \right) > P \left( w_{2} \right)$$时，$$w = w_{1}$$，分类为鲈鱼；当$$P \left( w_{2} \right) > P \left( w_{1} \right)$$时，$$w = w_{2}$$，分类为鲑鱼。若两类鱼出现概率相差不大，需更多特征信息，如鱼的长度、外表色泽深浅、嘴的位置和大小等。

### 与后验概率、似然概率的关系

-   先验概率是“由因求果”，后验概率是事情发生后，判断其由哪种原因引起，是“由果求因”，似然概率是给定类别$$w$$后，观察到变量$$x$$的概率$$P ( x | w )$$。

## 2. 最小错误率贝叶斯

### 似然概率（特征的类条件概率密度分布）

-   已知鲈鱼和鲑鱼关于亮度$$x$$的类条件概率分布情况，如$$p \left( x | w_{1} \right)$$和$$p \left( x | w_{2} \right)$$。

### 二类判决问题

-   已知鲈鱼和鲑鱼的先验概率$$p \left( w_{1} \right)$$和$$p \left( w_{2} \right)$$、亮度特征$$x$$的类条件概率密度$$p \left( x | w_{1} \right)$$和$$p \left( x | w_{2} \right)$$以及当前待分类鱼的亮度观测值$$x$$，判断观测值$$x$$属于鲈鱼和鲑鱼的概率情况，即$$p \left( w_{1} | x \right)$$和$$p \left( w_{2} | x \right)$$。合理规则为$$p \left( w_{1} | x \right) > p \left( w_{2} | x \right)$$时，$$w = w_{1}$$；$$p \left( w_{2} | x \right) > p \left( w_{1} | x \right)$$时，$$w = w_{2}$$。

### 贝叶斯公式

$$P \left( w_{i} | x \right) = \frac{P \left( x | w_{i} \right) P \left( w_{i} \right)}{P ( x )} = \frac{P \left( x | w_{i} \right) P \left( w_{i} \right)}{\sum_{i}^{} P \left( x | w_{i} \right) P \left( w_{i} \right)}$$，其中$$P o s t e r i o r = \frac{L i k e l i h o o d \times P r i o r}{E v i d e n c e}$$。$$P o s t e r i o r$$为$$P \left( w_{i} | x \right)$$，观测到具有$$x$$属性的示例或样本属于$$w_{i}$$的概率；$$L i k e l i h o o d$$为$$P \left( x | w_{i} \right)$$，似然，即第$$w_{i}$$类样本$$x$$属性或特征的分布情况；$$P r i o r$$为$$P \left( w_{i} \right)$$，先验概率；$$E v i d e n c e$$为“证据”因子，保证类别后验概率之和为$$1$$。$$w^{*} = a r g m a x \{ P \left( w_{i} | x \right) \}$$。

### 最小错误率贝叶斯与最大后验概率规则

-   最小错误率贝叶斯等价于最大后验概率规则，为最小化总体错误率，在每个样本上选择使错误最小的类别标记。对于$$w_{1}$$，$$P ( e r r o r | x ) = 1 - P \left( w_{1} | x \right) = P \left( w_{2} | x \right)$$；对于$$w_{2}$$，$$P ( e r r o r | x ) = 1 - P \left( w_{2} | x \right) = P \left( w_{1} | x \right)$$。最小化总体错误率即$$m i n P ( e r r o r ) \Rightarrow \int_{- \infty}^{\infty} m i n \{ P ( e r r o r | x ) \} p ( x ) d x$$，贝叶斯判定准则为选择使错误最小的类别标记。

### 举例（癌症诊断问题）

-   已知一批人中癌症病人和正常人的先验概率、试验呈阳性反应的条件概率，若某人呈阳性反应，通过计算$$P \left( w_{1} | x \right)$$和$$P \left( w_{2} | x \right)$$，判断其是否正常。计算过程为：先计算$$P \left( x | w_{1} \right) \cdot P \left( w_{1} \right)$$和$$P \left( x | w_{2} \right) \cdot P \left( w_{2} \right)$$，再根据贝叶斯公式计算$$P \left( w_{2} | x \right) = \frac{P \left( x | w_{2} \right) \cdot P \left( w_{2} \right)}{P \left( x | w_{1} \right) \cdot P \left( w_{1} \right) + P \left( x | w_{2} \right) \cdot P \left( w_{2} \right)}$$，进而得到$$P \left( w_{1} | x \right) = 1 - P \left( w_{2} | x \right)$$，比较两者大小得出结论。

## 3. 最小风险贝叶斯

### 问题定义

-   在给定样本$$x$$条件下，有一系列类别状态$$w_{1} , w_{2} , \cdots , w_{c}$$和一系列可能采取的行动（或决策）$$\alpha_{1} , \alpha_{2} , \cdots , \alpha_{a}$$，令$$\lambda_{i j} = \lambda \left( \alpha_{i} | w_{j} \right)$$表示当实际类别状态为$$w_{j}$$时，采取$$\alpha_{i}$$的行为会带来的损失，采取行动$$\alpha_{i}$$带来的条件风险$$R \left( \alpha_{i} | x \right) = \sum_{j = 1}^{c} \lambda \left( \alpha_{i} | w_{j} \right) P \left( w_{j} | x \right)$$。

### 最小风险判决步骤

1.  给定样本$$x$$条件下，计算各类后验概率$$P \left( w_{j} | x \right)$$。
2.  求各种判决的条件平均风险$$R \left( \alpha_{i} | x \right) = \sum \lambda \left( \alpha_{i} | w_{j} \right) P \left( w_{j} | x \right)$$。
3.  比较各种判决的条件平均风险，把样本$$x$$归属于条件平均风险最小的那一种判决$$\alpha^{*} = a r g m i n_{i} \{ R \left( \alpha_{i} | x \right) \}$$。

### 举例（癌症诊断问题）

-   已知风险矩阵，即不同判决行为在不同类别状态下的风险，以及各类后验概率$$P \left( w_{1} | x \right) = 0 . 677$$，$$P \left( w_{2} | x \right) = 0 . 323$$，计算不同判决行为的条件风险$$R \left( \alpha_{1} | x \right)$$和$$R \left( \alpha_{2} | x \right)$$，比较大小得出结论。计算过程为：根据风险矩阵和后验概率，计算$$R \left( \alpha_{1} | x \right) = \sum_{j = 1}^{2} \lambda \left( \alpha_{1} | w_{j} \right) \cdot P \left( w_{j} | x \right)$$和$$R \left( \alpha_{2} | x \right) = \sum_{j = 1}^{2} \lambda \left( \alpha_{2} | w_{j} \right) \cdot P \left( w_{j} | x \right)$$，比较两者大小，$$R \left( \alpha_{2} | x \right) < R \left( \alpha_{1} | x \right)$$，得出相应结论。

### 最小风险贝叶斯准则

-   判定为$$w_{1}$$，如果$$\lambda_{11} P \left( w_{1} | x \right) + \lambda_{12} P \left( w_{2} | x \right) < \lambda_{21} P \left( w_{1} | x \right) + \lambda_{22} P \left( w_{2} | x \right)$$，即$$\left( \lambda_{12} - \lambda_{22} \right) \frac{P \left( w_{2} , x \right)}{P ( x )} < \left( \lambda_{21} - \lambda_{11} \right) \frac{P \left( w_{1} , x \right)}{P ( x )}$$，进一步得到判定为$$w_{1}$$的条件为$$\frac{P \left( x | w_{1} \right)}{P \left( x | w_{2} \right)} > \frac{\lambda_{12} - \lambda_{22}}{\lambda_{21} - \lambda_{11}} \frac{P \left( w_{2} \right)}{P \left( w_{1} \right)}$$，比较似然比与阈值，阈值与观测$$x$$无关。

### 特殊情况（0 - 1损失）

-   当$$\lambda \left( \alpha_{i} | w_{j} \right) = \begin{cases}0 , & i = j \\ 1 , & i \neq j\end{cases}$$时，$$R \left( \alpha_{i} | x \right) = \sum_{j \neq i}^{} P \left( w_{j} | x \right) = 1 - P \left( w_{i} | x \right)$$，此时最小错误率贝叶斯是最小风险贝叶斯的一个特例。

### 贝叶斯决策论

-   贝叶斯决策论是概率框架下实施决策的基本方法，对于分类任务，在已知所有相关概率的理想情形下，考虑如何基于概率和误判损失选择最优类别标记。假设$$N$$种类别标记$$y = \{ c_{1} , c_{2} , \cdots , c_{N} \}$$，$$\lambda_{i j}$$是将真实标记为$$c_{j}$$的样本误分类为$$c_{i}$$的损失，基于后验概率$$P \left( c_{i} | x \right)$$可获得将样本$$x$$分类为$$c_{i}$$的期望损失（条件风险）$$R \left( c_{i} | x \right) = \sum_{j = 1}^{N} \lambda_{i j} P \left( c_{j} | x \right)$$，任务是寻找判定准则$$h : X \rightarrow Y$$以最小化总体风险$$R ( h ) = E_{x} \left\lbrack R \left( h ( x ) | x \right) \right\rbrack$$。贝叶斯判定准则为选择使条件风险$$R ( c | x )$$最小的类别标记，即$$h^{*} ( x ) = a r g m i n_{c \in y} R ( c | x )$$，$$h^{*}$$为贝叶斯最优分类器，$$R \left( h^{*} \right)$$为贝叶斯风险，$$1 - R \left( h^{*} \right)$$反映分类器所能达到的最好性能。

### 极大似然估计

-   估计$$P ( x | c )$$的常用策略是先假定概率分布形式，再参数估计。假设$$P ( x | c )$$具有确定形式被参数$$\theta_{c}$$唯一确定，利用训练集$$D$$估计参数$$\theta_{c}$$。令$$D_{c}$$表示训练集中第$$c$$类样本的组合集合，假设样本独立，则参数$$\theta_{c}$$对于数据集$$D_{c}$$的似然为$$P \left( D_{c} | \theta_{c} \right) = \prod_{x \in D_{c}}^{} P \left( x | \theta_{c} \right)$$，通常使用对数似然$$L L \left( \theta_{c} \right) = l o g P \left( D_{c} | \theta_{c} \right) = \sum_{x \in D_{c}}^{} l o g P \left( x | \theta_{c} \right)$$，参数$$\theta_{c}$$的极大似然估计$$\hat{\theta}_{c} = a r g m a x L L \left( \theta_{c} \right)$$。例如在连续属性情形下，假设概率密度函数$$p ( x | c ) \  N \left( \mu_{c} , \sigma_{c}^{2}  \right)$$，则参数$$\mu_{c}$$和$$\sigma_{c}^{2}$$的极大似然估计为$$\hat{\mu}_{c} = \frac{1}{\left| D_{c} \right|} \sum_{x \in D_{c}}^{} x$$，$$\hat{\sigma}_{c}^{2} = \frac{1}{\left| D_{c} \right|} \sum_{x \in D_{c}}^{} \left( x - \hat{\mu}_{c} \right) \left( x - \hat{\mu}_{c} \right)^{T}$$。

## 4. 朴素贝叶斯分类器

### 表达式

$$P ( c | x ) = \frac{P ( c ) P ( x | c )}{P ( x )} = \frac{P ( c )}{P ( x )} \prod_{i = 1}^{d} P \left( x_{i} | c \right)$$，贝叶斯判定准则为$$h_{n b} ( x ) = a r g m a x_{c \in y} P ( c ) \prod_{i = 1}^{d} P \left( x_{i} | c \right)$$。

### 训练过程

-   基于训练集$$D$$估计类先验概率$$P ( c )$$并为每个属性估计条件概率$$P \left( x_{i} | c \right)$$。对于离散属性，令$$D_{c , x_{i}}$$表示$$D_{c}$$中在第$$i$$个属性上取值为$$x_{i}$$的样本组成集合，则$$P \left( x_{i} | c \right) = \frac{\left| D_{c , x_{i}} \right|}{\left| D_{c} \right|}$$；对于连续属性，假定$$p \left( x_{i} | c \right) \  N \left( \mu_{c , i} , \sigma_{c , i}^{2}  \right)$$，则$$P \left( x_{i} | c \right) = \frac{1}{\sqrt{2 \pi} \sigma_{c , i}} \exp \left( - \frac{\left( x_{i} - \mu_{c , i} \right)^{2}}{2 \sigma_{c , i}^{2}} \right)$$。

### 举例（天气与打网球问题）

1.  给出训练样本，包括天气、气温、湿度、风、类别等信息。
2.  统计各类别样本数量，计算类先验概率$$P ( w )$$。
3.  分别统计不同属性在不同类别下的取值情况，计算条件概率$$P \left( x_{i} | w \right)$$。
4.  构建模型（查询表），包括$$P \left( x_{1} | w \right)$$、$$P \left( x_{2} | w \right)$$、$$P \left( x_{3} | w \right)$$、$$P \left( x_{4} | w \right)$$和$$P ( w )$$。
5.  对于测试样本，计算$$P ( x | w = 1 ) P ( w = 1 )$$和$$P ( x | w = 0 ) P ( w = 0 )$$，比较大小得出分类结果。若某个属性值在训练集中未与某个类同时出现，会导致计算问题，可采用拉普拉斯修正，修正公式为$$\hat{P} ( c ) = \frac{\left| D_{c} \right| + 1}{| D | + N}$$，$$\hat{P} \left( x_{i} | c \right) = \frac{\left| D_{c , x_{i}} \right| + 1}{\left| D_{c} \right| + N_{i}}$$。

## 5. 半朴素贝叶斯分类器

### 与朴素贝叶斯分类器的关系

-   朴素贝叶斯分类器假设属性条件独立，但现实中此假设难成立，半朴素贝叶斯分类器对其进行一定程度放松，考虑部分属性间相互依赖信息，不用计算完全联合概率。

### 独依赖估计（ODE）

-   假设每个属性在类别之外最多仅依赖一个其他属性，即$$P ( c | x ) \propto P ( c ) \prod_{i = 1}^{d} P \left( x_{i} | c , p a_{i} \right)$$，其中$$p a_{i}$$为属性$$x_{i}$$的父属性。关键是确定每个属性的父属性，可通过一些方法估计$$P \left( x_{i} | c , p a_{i} \right)$$。例如在西瓜数据集问题中，假设属性依赖关系，计算各类概率，比较得出分类结果。

### SPODE方法

-   假设所有属性都依赖于同一属性（超父），通过交叉验证等方法确定超父属性，形成SPODE方法。

### AODE分类器

-   基于集成学习机制，更为强大。尝试将每个属性作为超父构建SPODE，将具有足够训练数据支撑的SPODE集成起来作为最终结果，公式为$$P ( c | x ) \propto \sum_{i = 1 ; \left| D_{x_{i}} \geq m ' \right|}^{d} P \left( c , x_{i} \right) \prod_{j = 1}^{d} P \left( x_{j} | c , x_{i} \right)$$，并给出相关参数的估计公式。

### TAN分类器

-   在最大带权生成树算法基础上，通过计算任意两个属性之间的条件互信息构建最大带权生成树，确定属性间依赖关系，简约为特定图形。计算条件互信息公式为$$I \left( x_{i} , x_{j} | y \right) = \sum_{x_{i} , x_{j} ; c \in y}^{} P \left( x_{i} , x_{j} | c \right) \log \frac{P \left( x_{i} , x_{j} | c \right)}{P \left( x_{i} | c \right) P \left( x_{j} | c \right)}$$。例如在西瓜数据集问题中，构建最大带权生成树，确定属性父属性，计算相关概率，比较得出分类结果🔶1

# 概率图模型学习笔记

## 1. 概率模型与概率图模型

### 1.1概率模型

-   机器学习任务常需根据已有证据估计未知变量，概率模型提供了描述框架，将任务归结为计算变量的概率分布。
-   推断是利用已知变量推测未知变量的分布，核心是计算未知变量的条件分布$$P ( Y | O )$$。
-   生成式模型计算联合分布$$P ( Y , R , O )$$，判别式模型计算条件分布$$P ( Y , R | O )$$。
-   当变量取值简单时，直接计算概率的复杂度为指数级，需简洁表达变量关系的工具。

### 1.2概率图模型

-   是用图表达变量相关关系的概率模型，图的结点为随机变量（集合），边表示变量间的依赖关系。
-   分为有向图（如贝叶斯网，用于表示有显式因果关系的变量）和无向图（如马尔可夫网，用于表示变量间相关性但无显式因果关系）。

## 2. 贝叶斯网

### 2.1基本概念

-   借助有向无环图（DAG）刻画属性间依赖关系，用条件概率表（CPT）表述属性的联合概率分布。
-   一个贝叶斯网络由网络结构$$G$$（有向无环图，结点对应属性，有依赖关系的属性用边连接）和参数$$\Theta$$（包含每个属性的条件概率表）组成，即$$B = < G , \Theta >$$。

### 2.2结构特点

-   有效地表达了属性间的条件独立性，给定父结集，贝叶斯网假设每个属性与他的非后裔属性独立，属性的联合概率分布定义为$$P_{B} \left( x_{1} , x_{2} , \cdots , x_{d} \right) = \prod_{i = 1}^{d} P_{B} \left( x_{i} | \pi_{i} \right) = \prod_{i = 1}^{d} \theta_{x_{i} | \pi_{i}}$$。
-   贝叶斯网中三个变量之间有典型依赖关系，包括同父结构（给定父结点取值，子结点条件独立）、V型结构（给定子结点取值，父结点必不独立）、顺序结构（给定中间变量取值，两端变量条件独立）。

### 2.3分析方法

-   分析有向图中变量间的条件独立性，可使用“有向分离”（D - separation），通过将V型结构父结点相连、有向边变成无向边产生道德图，从道德图中去除变量集合$$Z$$后，若$$x$$和$$y$$分属两个连通分支，则$$x \perp y | z$$成立。

## 3. 马尔可夫模型

### 3.1基本概念

-   以随机过程视角，是状态选择仅与前一个状态有关的一阶模型，状态空间为$$S = \{ s_{1} , s_{2} , \cdots , s_{N} \}$$，状态序列为$$q_{1} , q_{2} , \cdots , q_{k} , \cdots$$，$$q_{i} \in S$$。

### 3.2模型组成

-   由状态转移概率矩阵$$A = \left( a_{i j} \right)$$（$$a_{i j} = P \left( s_{i} | s_{j} \right)$$）和初始概率向量$$\pi = \left( \pi_{i} \right)$$（$$\pi_{i} = P \left( s_{i} \right)$$）组成，即$$M = ( A , \pi )$$。

### 3.3计算示例

-   计算状态序列出现的概率，如状态序列{‘晴天’,‘晴天’,‘雨天’,‘雨天’}出现的概率为$$P ( ‘ 晴 天 ’ , ‘ 晴 天 ’ , ‘ 雨 天 ’ , ‘ 雨 天 ’ ) = P ( ‘ 雨 天 ’ | ‘ 雨 天 ’ ) P ( ‘ 雨 天 ’ | ‘ 晴 天 ’ ) P ( ‘ 晴 天 ’ | ‘ 晴 天 ’ ) P ( ‘ 晴 天 ’ ) = 0 . 3 \times 0 . 2 \times 0 . 8 \times 0 . 6 = 0 . 0288$$。

## 4. 隐马尔可夫模型

### 4.1基本概念

-   由一个隐藏的马尔可夫链生成不可观测的状态序列，状态序列中的每个状态生成一个观测，从而产生观测序列。状态空间为$$S = \{ s_{1} , s_{2} , \cdots , s_{N} \}$$，观察变量集合为$$V = \{ v_{1} , v_{2} , \cdots , v_{M} \}$$，观测序列为$$O = \left( o_{1} , o_{2} , \cdots , o_{K} \right)$$，状态序列为$$Q = \left( q_{1} , q_{2} , \cdots , q_{K} \right)$$。

### 4.2模型组成

-   包含状态转移概率矩阵$$A = \left( a_{i j} \right)$$（$$a_{i j} = P \left( s_{j} | s_{i} \right)$$）、观测概率矩阵$$B = \left( b_{j} ( k ) \right)$$（$$b_{j} ( k ) = P \left( v_{k} | s_{j} \right)$$）和初始状态概率向量$$\pi = \left( \pi_{i} \right)$$（$$\pi_{i} = P \left( s_{i} \right)$$），即$$M = ( A , B , \pi )$$。

### 4.3假设条件

-   齐次马尔可夫性假设：隐藏的马尔可夫链在任意时刻$$k$$的状态$$q_{k}$$只依赖于其前一时刻的状态$$q_{k - 1}$$，与其他时刻的状态及观测无关，也与时刻$$k$$无关，即$$P \left( q_{k} | q_{1} , o_{1} , q_{2} , o_{2} , \cdots , q_{k - 1} , o_{k - 1} \right) = P \left( q_{k} | q_{k - 1} \right)$$。
-   观测独立性假设：任意时刻的观测$$o_{k}$$只依赖于该时刻的马尔可夫链的状态$$q_{k}$$，与其他观测及状态无关，即$$P \left( o_{k} | q_{1} , o_{1} , q_{2} , o_{2} , \cdots , q_{k - 1} , o_{k - 1} , q_{k} , q_{k + 1} , o_{k + 1} \cdots , q_{K} , o_{K} \right) = P \left( o_{k} | q_{k} \right)$$。

### 4.4计算示例

-   计算观测序列出现的概率，如计算观测序列{‘晴天’,‘雨天’}出现的概率，需考虑所有可能的隐状态序列，分别计算其概率并求和。对于字母识别问题，通过比较不同HMM模型下观测序列的概率，评估模型与观测序列的匹配程度。

## 5. 三类基本问题

### 5.1评价问题

-   **前向算法**：定义前向概率$$\alpha_{k} ( i )$$为在时刻$$k$$，部分观测序列为$$\left( o_{1} , o_{2} , \cdots , o_{k} \right)$$且状态为$$q_{k} = s_{i}$$的概率。初始化$$\alpha_{1} ( i ) = b_{i} ( 1 ) \pi_{i}$$，向前递归$$\alpha_{k + 1} ( i ) = b_{i} ( k + 1 ) \sum_{j}^{} \alpha_{k} ( j ) a_{i j}$$，停止时$$P \left( o_{1} , o_{2} , \cdots , o_{K} \right) = \sum_{j}^{} \alpha_{K} ( j )$$，复杂度为$$N^{2} K$$。
-   **后向算法**：定义后向概率$$\beta_{k} ( i )$$为在时刻$$k$$，在状态为$$q_{k} = s_{i}$$条件下，部分观测序列为$$\left( o_{k + 1} , o_{k + 2} , \cdots , o_{K} \right)$$的概率。初始化$$\beta_{K} ( i ) = 1$$，向后递归$$\beta_{k} ( i ) = \sum_{j}^{} \beta_{k + 1} ( j ) b_{j} ( k + 1 ) a_{i j}$$，计算观测序列概率时$$P \left( o_{1} , o_{2} , \cdots , o_{K} \right) = \sum_{j}^{} \beta_{1} ( j ) b_{j} ( 1 ) \pi_{j}$$。

### 5.2解码问题

-   给定HMM模型$$M = ( A , B , \pi )$$和观测序列$$O = \left( o_{1} , o_{2} , \cdots , o_{K} \right)$$，用维特比算法计算产生此观测序列的最匹配状态序列$$Q = \left( q_{1} , q_{2} , \cdots , q_{K} \right)$$。定义变量$$\delta_{k} ( i )$$为在时刻$$k$$状态为$$q_{k} = s_{i}$$的所有单个路径中产生观测序列为$$\left( o_{1} , o_{2} , \cdots , o_{k} \right)$$的最大概率。初始化$$\delta_{1} ( i ) = \max \pi_{i} b_{i} ( 1 )$$，向前递归$$\delta_{k} ( i ) = \max_{j} \left\lbrack a_{j i} b_{i} ( k ) \delta_{k - 1} ( j ) \right\rbrack$$，停止时在$$K$$时刻选择最佳的结束路径$$\max_{i} \delta_{K} ( i )$$，该算法与前向递归类似，将$$\sum$$替换为$$\max$$并增加回溯。

### 5.3学习问题

-   **无隐状态信息**：已知观测序列$$O = \left( o_{1} , o_{2} , \cdots , o_{K} \right)$$，估计模型$$M = ( A , B , \pi )$$参数，使观测序列概率$$P ( O | M )$$最大，无最优算法，可使用iterative EM算法（如Baum - Welch算法）找到局部最优解。
-   **有隐状态信息**：可使用极大似然估计法估计参数，如$$a_{i j} = \frac{从 状 态 s_{i} 转 移 到 的 s_{j} 个 数}{所 有 从 状 态 s_{i} 出 发 的 个 数}$$，$$b_{j} ( m ) = \frac{在 状 态 s_{j} 观 测 到 的 观 测 值 为 v_{m} 的 次 数}{整 段 时 间 处 于 状 态 s_{j} 的 次 数}$$。Baum - Welch算法主要思想是通过计算期望个数来估计参数，定义$$\xi_{k} ( i , j )$$为给定观测序列下，在时间$$k$$是状态$$s_{i}$$和在时间$$k + 1$$是状态$$s_{j}$$的概率，然后据此计算$$a_{i j}$$、$$b_{j} ( m )$$和$$\pi_{i}$$。

# 图像特征描述子 - HOG与SIFT特征

## HOG特征

### 基本原理

1.  **图像梯度计算**
    -   图像$$f ( x , y )$$在点$$( x , y )$$的梯度是矢量，$$x$$方向梯度$$g_{x} = \frac{\partial f}{\partial x} = f ( x + 1 , y ) - f ( x , y )$$，$$y$$方向梯度$$g_{y} = \frac{\partial f}{\partial y} = f ( x , y + 1 ) - f ( x , y )$$，梯度强度$$g = \sqrt{g_{x}^{2} + g_{y}^{2}}$$，梯度方向$$\theta = \arctan \frac{g_{x}}{g_{y}}$$。
    -   当图像存在边缘时，梯度值较大；平滑部分灰度值变化小，梯度也小。$$x$$方向梯度强化垂直方向特征，$$y$$方向梯度强化水平方向特征。
2.  **HOG特征本质**
    -   HOG（Histogram of Oriented Gradient）是方向梯度直方图，是CV领域用于物体检测的特征描述子，本质是梯度的统计信息，能反映局部图像的边缘、形状等。由Dalal在CVPR 2005首次提出用于行人检测。

### 提取步骤

1.  **图像预处理**
    -   检查窗口大小根据应用场景选取（如$$64 \times 128$$），进行Gamma校正（调节图像对比度，减少光照不均和局部阴影，矫正过曝或曝光不足，$$\gamma = 0 . 67$$时扩展高光压缩暗调，$$\gamma = 1 . 5$$时相反）和灰度化。
2.  **计算图像梯度**
    -   计算水平和垂直方向梯度强度与方向（梯度图移除非显著性特征，加强显著特征，三通道彩色图取梯度强度最大通道的强度和对应方向）。
3.  **计算梯度直方图**
    -   以$$c e l l$$（如$$8 \times 8$$）为单位计算梯度直方图，每$$20 {^\circ}$$为一个单元，统计$$0 - 180 {^\circ}$$的梯度方向，得到$$9$$个值。根据梯度方向和强度更新直方图，如梯度方向$$80 {^\circ}$$强度$$2$$，则$$80 {^\circ}$$对应值加$$2$$；方向$$10 {^\circ}$$强度$$4$$，$$0 {^\circ}$$和$$20 {^\circ}$$对应值各加$$2$$；方向$$165 {^\circ}$$强度$$85$$，$$0 {^\circ}$$和$$160 {^\circ}$$的值分别加$$21 . 25$$和$$63 . 75$$（认为$$0 {^\circ}$$和$$180 {^\circ}$$等价）。优点是降低计算量且对光照环境变化鲁棒。
4.  **归一化**
    -   以$$b l o c k$$（如$$16 \times 16$$，$$1$$个$$b l o c k = 4$$个$$c e l l$$）为单位进行归一化，方法是向量每个值除以向量的模（如$$( 128 , 64 , 32 )$$归一化后为$$( 0 . 87 , 0 . 43 , 0 . 22 )$$），使特征描述子不受光照变化影响。
5.  **组合成HOG特征向量**
    -   将所有$$b l o c k$$的归一化梯度直方图组合成HOG特征向量，维度为$$36 \times 105 = 3780$$（$$64 \times 128$$大小图像，$$7 \times 15 = 105$$个$$b l o c k$$，每个$$b l o c k$$有$$36$$个特征）。梯度直方图主导方向捕捉人的形状（如躯干和腿部）。

### 特征特性

-   HOG特征描述子通过统计图像局部区域梯度方向信息表征局部图像区域，类似SIFT特征描述子。但HOG不选取主方向，无旋转梯度方向直方图，本身不具旋转不变性（通过不同旋转方向训练样本实现）和尺度不变性（通过改变检测图像大小实现）。

## SIFT特征

### 基本原理

1.  **SIFT定义与特性**
    -   SIFT（Scale Invariant Feature Transform）即尺度不变特征转换，用于图像处理，具有尺度不变性，对物体尺度变化、刚体变换、光照强度和遮挡有较好稳定性，可检测关键点，是局部特征描述算子。由British Columbia大学大卫·劳伊（David G. Lowe）教授提出并完善。
2.  **解决问题与应用**
    -   解决目标在不同状态（旋转、缩放、平移、遮挡等）和环境（光照、仿射/投影变换、杂物场景、噪声等）下的图像配准/目标识别跟踪问题，应用于全景图像拼接、物体识别、三维重建等。
3.  **特征匹配流程**
    -   包含找出显著特征点（Feature Detect）、描述特征点（Feature Descriptor）、比较描述相似程度判断是否为同一特征（Feature Match）三步，是计算机视觉应用（如图像配准、摄像机跟踪、三维重建、物体识别、人脸识别）的基础。

### 算法实现细节

1.  **构建尺度空间**
    -   尺度空间$$L ( x , y , \sigma ) = G ( x , y , \sigma ) * I ( x , y )$$（$$G ( x , y , \sigma ) = \frac{1}{2 \pi \sigma^{2}} \exp \left( - \frac{\left( x - x_{i} \right)^{2} + \left( y - y_{i} \right)^{2}}{2 \sigma^{2}} \right)$$），$$\sigma$$越小图像越清晰，反之越模糊。早期多尺度用高斯金字塔表述，构建分两步：对图像高斯平滑/模糊，再降采样得到尺寸缩小图像，降采样基础上加高斯滤波体现尺度连续性。一组图像（octave）包括多层（interval）图像。
    -   传统SIFT通过高斯差分尺度空间（DOG）提取特征点，DOG函数$$D ( x , y , \sigma ) = L ( x , y , k \sigma ) - L ( x , y , \sigma )$$，计算效率高，是尺度归一化LoG的近似，尺度稳定性好。DOG尺度空间通过相邻尺度高斯平滑后图像相减得到。
2.  **特征点定位**
    -   特征点是不同尺度空间图像下具方向信息的局部极值点，具有尺度、方向、大小特征。先在DOG尺度空间通过点与同尺度$$8$$个相邻点及上下相邻尺度$$9 \times 2$$个点共$$26$$个点比较检测极值点（一个点在DOG尺度空间的$$26$$个邻域中是最大或最小值时为特征点），再通过曲线拟合DOG函数精确定位（求导令为$$0$$得到精确位置偏移量，若偏移量大于$$0 . 5$$更换特征点重复流程），最后去除不稳定特征点（将精度点值代入泰勒展开式，$$\left| D \left( \hat{X} \right) \right| \geq 0 . 03$$保留，否则丢弃；去除DoG算子产生的边缘响应点，通过$$2 \times 2$$的Hessian矩阵$$H$$判断，$$\frac{T r^{2} ( H )}{D e t ( H )} = \frac{\left( \lambda_{1} / \lambda_{2} + 1 \right)^{2}}{\lambda_{1} / \lambda_{2}} \leq \frac{( 10 + 1 )^{2}}{10}$$保留，否则丢弃）。
3.  **特征点主方向确定**
    -   计算以特征点为中心的邻域（窗口）内像素梯度方向和幅值，统计梯度方向直方图确定主方向（峰值方向），特殊情况（如最大峰值小于$$80 \%$$第二峰值）时可能有多个主方向或需进一步处理。
4.  **特征描述**
    -   以特征点为中心根据主方向取$$4$$个$$4 \times 4$$窗口，对梯度幅值图高斯平滑，在每个$$4 \times 4$$子区域绘制$$8$$个方向的梯度幅值加权直方图形成种子点，实际应用中为增强匹配稳健性，用$$16$$个种子点描述，得到$$4 \times 4 \times 8 = 128$$维SIFT特征向量。
5.  **特征匹配**
    -   基本假设是通过特征描述子间欧式距离判断匹配程度，距离越小匹配度越高。基本思路是距离小于阈值则匹配成功，常用距离度量函数有固定阈值、最近邻、最近邻比值法（距离最近和次近特征点距离比值小于阈值时，将最近点作为匹配点）。

### 算法扩展与改进

-   SIFT算法存在实时性不高、特征点少、对边缘模糊目标提取特征点不准确等缺陷，后续有PCA - SIFT（利用主成分分析降维化简SIFT算子描述子）、CSIFT（针对彩色图像提取不变特征）、SURF（计算量小、速度快，特征点与SIFT几乎相同）等改进算法。SIFT、PCA - SIFT与SURF在时间、尺度、旋转、模糊、光照、仿射等方面性能各有优劣。

# 神经网络基础笔记

## 神经网络发展史

### 感知器时代（1950s - 1970s）

-   1958年，感知机（Perceptron）被提出，是线性二分类器，有两层神经元，是首个能学习权重并进行简单分类的人工神经网络，具备现今神经网络主要构件与思想，但无法解决异或问题（数据线性不可分问题），导致神经网络进入第一次寒冬。
-   1980s，多层感知机出现，与现在的深度神经网络（DNN）无显著区别。

### BP算法时代（1986 - 1998）

-   1986年，反向传播（BP）算法提出，引入激活函数sigmoid，解决感知机遗留问题，实现多层网络BP算法，是应用最广泛的训练更新算法，但一般大于3个隐藏层作用不大。
-   1989年，提出1个隐藏层“足够好”，引发对深度必要性的思考。LeNet被提出，为后续深度学习研究奠基，促进卷积神经网络发展。
-   1991年，BP算法被指存在梯度消失和爆炸问题，神经网络发展受冷遇。
-   1990s，支持向量机（SVM）出现，理论完备、可解释性强，解决神经网络部分遗留问题，使其再次冷寂。1997年，长短期记忆网络（LSTM）被提出。1998年，基于LeNet的LeNet - 5提出，实现手写数字识别，在MNIST数据集上表现受关注。

### 深度学习时代（2006 - 至今）

-   2000s，GPU和分布式计算助力计算机算力提升，为深度学习奠定基础。
-   2006年，深度信念网络（DBN）取得突破，利用无监督预训练解决深层神经网络梯度消失问题，深度学习时代来临。
-   2009年，受限玻尔兹曼机（RBM）提出，可学习数据集概率分布。2011年，开始在语音识别领域流行，同年ReLu激活函数被提出，解决Sigmoid函数梯度消失问题，至今广泛应用。
-   2012年，AlexNet在ImageNet图像分类大赛中获胜，推动深度学习快速发展。
-   2014年，VGGNet、GoogLeNet被提出，GoogLeNet提出inception模块学习不同尺度特征；同年生成对抗网络（GAN）被提出。
-   2015年，残差网络（ResNet）提出，残差思想突破，在ImageNet比赛中夺冠，图像识别超越人类水平。
-   2016年，轻量级卷积神经网络SqueezeNet、Alpha GO击败围棋世界冠军、在语音识别取得进展。
-   2017年，ResNet升级版DenseNet、Google提出Transformer模型（基于自注意力机制，为后续BERT、GPT等模型奠基）被提出。
-   2018年，Google提出BERT模型，显著提升多项自然语言处理（NLP）任务性能。
-   2019年，OpenAI提出GPT - 2模型引发关注，DeepMind的AlphaStar在星际争霸II中战胜人类职业选手，AlphaFold解决蛋白质折叠问题。
-   2020年，DeepMind的AlphaFold解决蛋白质折叠问题，获生物学领域突破。
-   2021年，OpenAI发布图像生成模型DALL·E。
-   2022年，OpenAI发布GPT - 3.5（ChatGPT），引发大模型浪潮。
-   2023年，OpenAI发布ChatGLM，具备对话能力的聊天机器人。
-   2024年，OpenAI发布Sora，诺贝尔物理学奖、化学奖颁发给神经网络领域相关科学家。

## 神经元和神经网络

### 深度学习步骤

1.  **神经网络（Neural Network）**：由输入层、隐藏层（可多层）和输出层构成，不同连接方式形成不同网络结构，网络参数包括所有权重和偏置。从线性回归到逻辑回归，逻辑回归输出在0 - 1之间，通过激活函数（如Sigmoid函数）实现，常用全连接前馈网络，可使用GPU加速计算。
2.  **模型评估（Goodness of Function）**：通过损失函数（如hinge loss、cross entropy loss等）评估模型好坏。
3.  **选择最优函数（Pick the Best Function）**：使用梯度下降法（包括随机梯度下降）等优化算法，通过计算损失函数对参数的梯度，更新参数找到最优函数。

### 神经网络结构与计算

-   神经网络通过神经元连接，每个神经元接收输入，乘以权重并加上偏置后，经过激活函数处理得到输出，多层神经元连接形成网络结构。
-   全连接前馈网络中，输入数据依次经过各隐藏层处理后到达输出层，输入层神经元数量取决于输入数据维度，输出层神经元数量取决于任务类别数，隐藏层数量和每层神经元数量需通过试验、直觉或特定算法确定（如卷积神经网络CNN可自动确定部分结构），也可使用神经架构搜索（NAS）自动寻找最优结构，NAS包括搜索空间、搜索策略和性能评估策略三部分。

### 示例应用（以手写数字识别为例）

-   输入为256维向量（16x16像素图像，有墨为1，无墨为0），输出为10维向量（代表0 - 9十个数字的概率），需确定网络结构，使函数集中包含能准确分类的函数。

## 反向传播算法

### 梯度下降法

1.  梯度下降法是深度学习中优化神经网络参数的常用方法，通过计算损失函数对参数的梯度，沿梯度反方向更新参数以最小化损失函数。
2.  计算梯度时，需计算损失函数对每个权重（$$w$$）和偏置（$$b$$）的偏导数（$$\frac{\partial L}{\partial w}$$、$$\frac{\partial L}{\partial b}$$），形成梯度向量（$$\nabla L$$），然后根据学习率（$$\eta$$）更新参数（$$\theta^{n e w} = \theta^{o l d} - \eta \nabla L$$），其中$$\theta$$表示网络参数（$$w$$和$$b$$）。

### 反向传播（Backpropagation）

1.  反向传播是计算神经网络中损失函数对权重和偏置梯度的有效方法，许多深度学习框架（如TensorFlow、PyTorch等）都基于此实现自动求导。
2.  其原理基于链式法则，对于多层神经网络，从输出层开始，反向计算每层的梯度。计算过程包括前向传播（Forward pass）计算各层输出和激活函数输入（$$z$$），以及反向传播（Backward pass）计算损失函数对$$z$$的梯度（$$\frac{\partial C}{\partial z}$$），进而计算对权重和偏置的梯度（$$\frac{\partial C}{\partial w}$$、$$\frac{\partial C}{\partial b}$$）。
3.  前向传播计算参数$$z$$（如$$z = x_{1} w_{1} + x_{2} w_{2} + b$$）对权重$$w$$的偏导数（$$\frac{\partial z}{\partial w}$$），其值等于与该权重相连输入的数值（如$$\frac{\partial z}{\partial w_{1}} = x_{1}$$）。
4.  反向传播计算损失函数对$$z$$的梯度（$$\frac{\partial C}{\partial z}$$），根据链式法则，对于输出层，$$\frac{\partial C}{\partial z} = \frac{\partial y}{\partial z} \frac{\partial C}{\partial y}$$（如使用softmax和交叉熵损失函数时）；对于非输出层，通过递归计算（如$$\frac{\partial C}{\partial z} = \sigma ' ( z ) \left\lbrack w_{3} \frac{\partial C}{\partial z '} + w_{4} \frac{\partial C}{\partial z ''} \right\rbrack$$），其中$$\sigma ' ( z )$$为激活函数的导数，$$z '$$、$$z ''$$为后续层的输入。

## CNN简介

### 基本概念

1.  卷积神经网络（CNN）是含卷积计算的前馈神经网络，用于处理具有网格结构数据（如图像、音频），相比全连接网络，有局部连接、权重共享和池化层三个结构特性。
2.  局部连接（感受野）：神经元仅考虑输入图像的局部区域（感受野），不同感受野可重叠，多个神经元可共用同一感受野，感受野大小影响对原始图像特征的感知范围，常见填充方法有填充均值、0或边界值，步长决定感受野移动步幅。
3.  权重共享：不同感受野的神经元可共享参数（通过卷积核或滤波器实现），同一感受野内神经元一般不共享参数，权重共享减少参数数量，但也限制网络灵活性，不同图片因输入不同，即使共享参数输出也不同。
4.  池化层（Pooling）：如最大池化（Max Pooling），对特征图进行下采样，减少数据量，保留主要特征，不改变图像对象特征，池化操作可缩小特征图尺寸，降低计算量，实际中常与卷积层交替使用，但在处理精细数据（如围棋棋盘）时可能不适用（如AlphaGo未使用池化层）。

### 工作原理

1.  卷积层：通过卷积核（滤波器）在输入图像上滑动进行卷积操作，每个卷积核检测一种小模式（特征），输出特征图，多个卷积核可提取多种特征，卷积层叠加深可扩大网络感受野，检测更大范围特征。
2.  全连接层：通常在网络末端，将卷积层和池化层提取的特征映射到最终输出（如分类任务的类别概率）。

### 应用场景

1.  图像分类：如在ImageNet数据集上的应用，通过卷积层提取图像特征，池化层下采样，全连接层分类，使用交叉熵损失函数和softmax函数计算预测概率和损失。
2.  围棋：AlphaGo使用CNN，因围棋存在局部特征和模式重复，适合用CNN处理，输入19x19x48图像（48个特征平面），卷积层提取特征，最后一层卷积结合softmax预测落子位置，虽围棋未用池化层，但CNN仍表现出色。
3.  其他：在语音处理、自然语言处理等领域也有应用，如语音中的卷积层处理音频特征，自然语言处理中通过卷积操作提取文本特征。
