# 神经网络基础笔记
## 神经网络发展史
### 感知器时代（1950s - 1970s）
- 1958年，感知机（Perceptron）被提出，是线性二分类器，有两层神经元，是首个能学习权重并进行简单分类的人工神经网络，具备现今神经网络主要构件与思想，但无法解决异或问题（数据线性不可分问题），导致神经网络进入第一次寒冬。
- 1980s，多层感知机出现，与现在的深度神经网络（DNN）无显著区别。
### BP算法时代（1986 - 1998）
- 1986年，反向传播（BP）算法提出，引入激活函数sigmoid，解决感知机遗留问题，实现多层网络BP算法，是应用最广泛的训练更新算法，但一般大于3个隐藏层作用不大。
- 1989年，提出1个隐藏层“足够好”，引发对深度必要性的思考。LeNet被提出，为后续深度学习研究奠基，促进卷积神经网络发展。
- 1991年，BP算法被指存在梯度消失和爆炸问题，神经网络发展受冷遇。
- 1990s，支持向量机（SVM）出现，理论完备、可解释性强，解决神经网络部分遗留问题，使其再次冷寂。1997年，长短期记忆网络（LSTM）被提出。1998年，基于LeNet的LeNet - 5提出，实现手写数字识别，在MNIST数据集上表现受关注。
### 深度学习时代（2006 - 至今）
- 2000s，GPU和分布式计算助力计算机算力提升，为深度学习奠定基础。
- 2006年，深度信念网络（DBN）取得突破，利用无监督预训练解决深层神经网络梯度消失问题，深度学习时代来临。
- 2009年，受限玻尔兹曼机（RBM）提出，可学习数据集概率分布。2011年，开始在语音识别领域流行，同年ReLu激活函数被提出，解决Sigmoid函数梯度消失问题，至今广泛应用。
- 2012年，AlexNet在ImageNet图像分类大赛中获胜，推动深度学习快速发展。
- 2014年，VGGNet、GoogLeNet被提出，GoogLeNet提出inception模块学习不同尺度特征；同年生成对抗网络（GAN）被提出。
- 2015年，残差网络（ResNet）提出，残差思想突破，在ImageNet比赛中夺冠，图像识别超越人类水平。
- 2016年，轻量级卷积神经网络SqueezeNet、Alpha GO击败围棋世界冠军、在语音识别取得进展。
- 2017年，ResNet升级版DenseNet、Google提出Transformer模型（基于自注意力机制，为后续BERT、GPT等模型奠基）被提出。
- 2018年，Google提出BERT模型，显著提升多项自然语言处理（NLP）任务性能。
- 2019年，OpenAI提出GPT - 2模型引发关注，DeepMind的AlphaStar在星际争霸II中战胜人类职业选手，AlphaFold解决蛋白质折叠问题。
- 2020年，DeepMind的AlphaFold解决蛋白质折叠问题，获生物学领域突破。
- 2021年，OpenAI发布图像生成模型DALL·E。
- 2022年，OpenAI发布GPT - 3.5（ChatGPT），引发大模型浪潮。
- 2023年，OpenAI发布ChatGLM，具备对话能力的聊天机器人。
- 2024年，OpenAI发布Sora，诺贝尔物理学奖、化学奖颁发给神经网络领域相关科学家。

## 神经元和神经网络
### 深度学习步骤
1. **神经网络（Neural Network）**：由输入层、隐藏层（可多层）和输出层构成，不同连接方式形成不同网络结构，网络参数包括所有权重和偏置。从线性回归到逻辑回归，逻辑回归输出在0 - 1之间，通过激活函数（如Sigmoid函数）实现，常用全连接前馈网络，可使用GPU加速计算。
2. **模型评估（Goodness of Function）**：通过损失函数（如hinge loss、cross entropy loss等）评估模型好坏。
3. **选择最优函数（Pick the Best Function）**：使用梯度下降法（包括随机梯度下降）等优化算法，通过计算损失函数对参数的梯度，更新参数找到最优函数。
### 神经网络结构与计算
- 神经网络通过神经元连接，每个神经元接收输入，乘以权重并加上偏置后，经过激活函数处理得到输出，多层神经元连接形成网络结构。
- 全连接前馈网络中，输入数据依次经过各隐藏层处理后到达输出层，输入层神经元数量取决于输入数据维度，输出层神经元数量取决于任务类别数，隐藏层数量和每层神经元数量需通过试验、直觉或特定算法确定（如卷积神经网络CNN可自动确定部分结构），也可使用神经架构搜索（NAS）自动寻找最优结构，NAS包括搜索空间、搜索策略和性能评估策略三部分。
### 示例应用（以手写数字识别为例）
- 输入为256维向量（16x16像素图像，有墨为1，无墨为0），输出为10维向量（代表0 - 9十个数字的概率），需确定网络结构，使函数集中包含能准确分类的函数。

## 反向传播算法
### 梯度下降法
1. 梯度下降法是深度学习中优化神经网络参数的常用方法，通过计算损失函数对参数的梯度，沿梯度反方向更新参数以最小化损失函数。
2. 计算梯度时，需计算损失函数对每个权重（\(w\)）和偏置（\(b\)）的偏导数（\(\frac{\partial L}{\partial w}\)、\(\frac{\partial L}{\partial b}\)），形成梯度向量（\(\nabla L\)），然后根据学习率（\(\eta\)）更新参数（\(\theta^{new}=\theta^{old}-\eta\nabla L\)），其中\(\theta\)表示网络参数（\(w\)和\(b\)）。
### 反向传播（Backpropagation）
1. 反向传播是计算神经网络中损失函数对权重和偏置梯度的有效方法，许多深度学习框架（如TensorFlow、PyTorch等）都基于此实现自动求导。
2. 其原理基于链式法则，对于多层神经网络，从输出层开始，反向计算每层的梯度。计算过程包括前向传播（Forward pass）计算各层输出和激活函数输入（\(z\)），以及反向传播（Backward pass）计算损失函数对\(z\)的梯度（\(\frac{\partial C}{\partial z}\)），进而计算对权重和偏置的梯度（\(\frac{\partial C}{\partial w}\)、\(\frac{\partial C}{\partial b}\)）。
3. 前向传播计算参数\(z\)（如\(z = x_1w_1 + x_2w_2 + b\)）对权重\(w\)的偏导数（\(\frac{\partial z}{\partial w}\)），其值等于与该权重相连输入的数值（如\(\frac{\partial z}{\partial w_1}=x_1\)）。
4. 反向传播计算损失函数对\(z\)的梯度（\(\frac{\partial C}{\partial z}\)），根据链式法则，对于输出层，\(\frac{\partial C}{\partial z}=\frac{\partial y}{\partial z}\frac{\partial C}{\partial y}\)（如使用softmax和交叉熵损失函数时）；对于非输出层，通过递归计算（如\(\frac{\partial C}{\partial z}=\sigma'(z)[w_3\frac{\partial C}{\partial z'}+w_4\frac{\partial C}{\partial z''}]\)），其中\(\sigma'(z)\)为激活函数的导数，\(z'\)、\(z''\)为后续层的输入。

## CNN简介
### 基本概念
1. 卷积神经网络（CNN）是含卷积计算的前馈神经网络，用于处理具有网格结构数据（如图像、音频），相比全连接网络，有局部连接、权重共享和池化层三个结构特性。
2. 局部连接（感受野）：神经元仅考虑输入图像的局部区域（感受野），不同感受野可重叠，多个神经元可共用同一感受野，感受野大小影响对原始图像特征的感知范围，常见填充方法有填充均值、0或边界值，步长决定感受野移动步幅。
3. 权重共享：不同感受野的神经元可共享参数（通过卷积核或滤波器实现），同一感受野内神经元一般不共享参数，权重共享减少参数数量，但也限制网络灵活性，不同图片因输入不同，即使共享参数输出也不同。
4. 池化层（Pooling）：如最大池化（Max Pooling），对特征图进行下采样，减少数据量，保留主要特征，不改变图像对象特征，池化操作可缩小特征图尺寸，降低计算量，实际中常与卷积层交替使用，但在处理精细数据（如围棋棋盘）时可能不适用（如AlphaGo未使用池化层）。
### 工作原理
1. 卷积层：通过卷积核（滤波器）在输入图像上滑动进行卷积操作，每个卷积核检测一种小模式（特征），输出特征图，多个卷积核可提取多种特征，卷积层叠加深可扩大网络感受野，检测更大范围特征。
2. 全连接层：通常在网络末端，将卷积层和池化层提取的特征映射到最终输出（如分类任务的类别概率）。
### 应用场景
1. 图像分类：如在ImageNet数据集上的应用，通过卷积层提取图像特征，池化层下采样，全连接层分类，使用交叉熵损失函数和softmax函数计算预测概率和损失。
2. 围棋：AlphaGo使用CNN，因围棋存在局部特征和模式重复，适合用CNN处理，输入19x19x48图像（48个特征平面），卷积层提取特征，最后一层卷积结合softmax预测落子位置，虽围棋未用池化层，但CNN仍表现出色。
3. 其他：在语音处理、自然语言处理等领域也有应用，如语音中的卷积层处理音频特征，自然语言处理中通过卷积操作提取文本特征。